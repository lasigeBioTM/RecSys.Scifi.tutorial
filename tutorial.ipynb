{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "896af332",
   "metadata": {},
   "source": [
    "# Tutorial: Creating Recommender Systems Datasets in Scientific Fields\n",
    "\n",
    "- [1. Data retrieval and cleaning](##1.-Data-retrieval-and-cleaning)\n",
    "    - [1.1. Import libraries](###1.1.-Import-libraries)\n",
    "    - [1.2. Retrieve CORD-19](###1.2.-Retrieve-CORD-19)\n",
    "    - [1.3. Exploring the articles of the dataset](###1.3.-Exploring-the-articles-of-the-dataset)\n",
    "    - [1.4. Selecting a sample of articles to build our scientific recommendation dataset\n",
    "](###1.4.-Selecting-a-sample-of-articles-to-build-our-scientific-recommendation-dataset)\n",
    "- [2. Named Entity Recognition (NER) + Named Entity Linking (NEL)](#2.)\n",
    "    - [2.1. Import libraries](###2.1.-Import-libraries)\n",
    "    - [2.2. Configure MER](#2.2.-Configure-MER)\n",
    "    - [2.3. Extract the entities in a single file](###2.3.-Extract-the-entities-in-a-single-file)\n",
    "    - [2.4. Create entity files](###2.4.-Create-entity-files)\n",
    "- [3. Creating the recommendation dataset](##3.-Creating-the-recommendation-dataset) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380363b1",
   "metadata": {},
   "source": [
    "## 1. Data retrieval + cleaning\n",
    "\n",
    "**Objective**: To retrieve the [COVID-19 Open Research Dataset (CORD-19)](https://www.semanticscholar.org/cord19) and to select a sample of complete English articles (authors' info, title, body text) to build a scientific recommendation dataset.\n",
    "\n",
    "CORD-19 includes coronavirus-related research articles extracted from several sources, such as PubMed, bioRxiv, medRxiv, WHO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2336226d",
   "metadata": {},
   "source": [
    "### 1.1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0844dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c96da7",
   "metadata": {},
   "source": [
    "### 1.2. Retrieve CORD-19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6726700e",
   "metadata": {},
   "source": [
    "CORD-19 is a large dataset, so in this tutorial we are going to use a smaller version of the dataset. \n",
    "This version is located under the directory \"cord19_small\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77861be",
   "metadata": {},
   "source": [
    "However, if you want to retrieve the entire dataset, you can run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305a30d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'cord-19_2020-05-12.tar.gz'\n",
    "\n",
    "url = 'https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/' + version\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "os.system('tar -cvf cord-19_2020-05-12.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f7272",
   "metadata": {},
   "source": [
    "Let's explore the contents of the dataset directory, particularly, the metadata file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26501ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'cord19_small/'\n",
    "metadata_filepath = dataset_dir + 'metadata.csv'\n",
    "\n",
    "metadata = pd.read_csv(metadata_filepath, sep = ',', quotechar = '\"',  encoding = 'utf-8', dtype=str) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0cf98f",
   "metadata": {},
   "source": [
    "Now we have a DataFrame with the contents of the metadata file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8589d06",
   "metadata": {},
   "source": [
    "To print column names and first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10e6f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be54e22c",
   "metadata": {},
   "source": [
    "To access individual rows/articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38853ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37c7b7d",
   "metadata": {},
   "source": [
    "To access the individual column title':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ceea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4d10d0",
   "metadata": {},
   "source": [
    "Let's check the summary statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9978f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22220fff",
   "metadata": {},
   "source": [
    "By looking at the statistics, we can see that the number of records with data for the column 'cord_uid' is higher than the number of records with data for the column 'authors'. \n",
    "\n",
    "For our dataset, we only want to include articles with the following characteristics:\n",
    "- authors' information\n",
    "- available title\n",
    "- available body text\n",
    "- article text expressed in English\n",
    "- non-duplicate articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939fcb80",
   "metadata": {},
   "source": [
    "### 1.3. Exploring the articles of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d806dd60",
   "metadata": {},
   "source": [
    "Let's consider the first article appearing in the metadata file.\n",
    "To check if information about the article's author is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbef2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.loc[0]['authors']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220d416b",
   "metadata": {},
   "source": [
    "To check if there is an available title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b6792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.loc[0]['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f2a37a",
   "metadata": {},
   "source": [
    "We can see that the title is expressed in English, but it would not be efficient to check the language of every article in the dataset, so we will apply a language detection tool, the Python library [Googletrans](https://pypi.org/project/googletrans/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e434d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "\n",
    "title1 = metadata.loc[0]['title']\n",
    "\n",
    "title1_lang = translator.detect(title).lang\n",
    "\n",
    "print(\"Title:\", title1, \"\\nLanguage:\", title1_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27da7df",
   "metadata": {},
   "source": [
    "The tool detects English as the language of the title.\n",
    "\n",
    "Let's check another article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858859a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "title2 = metadata.loc[107]['title']\n",
    "print(metadata.loc[107])\n",
    "title2_lang = translator.detect(title2).lang\n",
    "print(title2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bed2cc",
   "metadata": {},
   "source": [
    "In this case, the tool detects german ('de') as the language of the title."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fd7b45",
   "metadata": {},
   "source": [
    "Now, we want to check if there is an available abstract for the first article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd4f921",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.loc[0]['abstract']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c19af0a",
   "metadata": {},
   "source": [
    "The metadata file does not contain the article's text besides abstract and title, but we can access the file associated with the article using the provided information in the columns 'pdf_json_files' or 'pmc_json_files':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f952a8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.loc[0]['pdf_json_files']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108098d0",
   "metadata": {},
   "source": [
    "Let's open the file, which is in [JSON](https://www.json.org/json-en.html) format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b017d3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "article1_filepath = dataset_dir + metadata.loc[0]['pdf_json_files']\n",
    "\n",
    "with open(article1_filepath, encoding='utf-8') as article1_file:\n",
    "    article1_data = json.load(article1_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabad8a1",
   "metadata": {},
   "source": [
    "Now we have the file content stored in a dictionary with the following keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbac510",
   "metadata": {},
   "outputs": [],
   "source": [
    "article1_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9225aac4",
   "metadata": {},
   "source": [
    "The article content is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86544caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "article1_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d0802d",
   "metadata": {},
   "source": [
    "Let's check the body text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31053239",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_text = article1_data['body_text']\n",
    "body_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ea2ca1",
   "metadata": {},
   "source": [
    "We have a list of dictionaries, each dictionary is a paragraph beloning to a given section of the article. We want to join the scattered text in a single string: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b8874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "article1_text = str()\n",
    "\n",
    "for paragraph in body_text:\n",
    "    article1_text += paragraph['text'] + '\\n'\n",
    "\n",
    "print(article1_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c282e3f5",
   "metadata": {},
   "source": [
    "All good! We will include this article in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadd9a79",
   "metadata": {},
   "source": [
    "### 1.4. Selecting a sample of articles to build our scientific recommendation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9685e6",
   "metadata": {},
   "source": [
    "Instead of repating each operation for each file individually, let us adapt our code to automatically select a sample containing 100 preprocessed articles. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704825d0",
   "metadata": {},
   "source": [
    "First, create the output directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ef8d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'cord19_sample/'\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a30aaba",
   "metadata": {},
   "source": [
    "Then, initiallize the necessary variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5524d8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '2020-05-19/'\n",
    "metadata_filepath = dataset_dir + 'metadata.csv'\n",
    "max_articles = 100\n",
    "valid_articles_count = int()\n",
    "out_articles_ids = list()\n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe824fd6",
   "metadata": {},
   "source": [
    "Open the metadata file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a194667",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(metadata_filepath, sep = ',', quotechar = '\"',  encoding = 'utf-8', dtype=str) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72805dd",
   "metadata": {},
   "source": [
    "Then iterate over the records in the metadata file and choose only the relevant ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a23be5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_articles_count = int()\n",
    "\n",
    "for index, record in metadata.iterrows():\n",
    "    \n",
    "    if valid_articles_count <= max_articles:\n",
    "        \n",
    "        if record['pubmed_id'] not in out_articles_ids:\n",
    "\n",
    "            if record['authors']:    \n",
    "     \n",
    "                if record['title']:\n",
    "                    title = record['title']\n",
    "\n",
    "                    title_lang = translator.detect(title).lang\n",
    "                    article_filepath = record['pdf_json_files']\n",
    "                    \n",
    "                    if title_lang == 'en'  \\\n",
    "                        and type(article_filepath) != float  \\\n",
    "                        and article_filepath.count(\"document\") == 1:\n",
    "                            \n",
    "                        article_filepath_up = dataset_dir + record['pdf_json_files']\n",
    "                       \n",
    "                        with open(article_filepath_up, encoding='utf-8') as article_file:\n",
    "                            article_data = json.load(article_file)\n",
    "                        \n",
    "                        if 'body_text' in article_data.keys():\n",
    "                            command = 'cp '  \\\n",
    "                                     + article_filepath + ' ' \\\n",
    "                                     + out_dir  \\\n",
    "                                     + record['sha'] + '.json'\n",
    "                            valid_articles_count += 1\n",
    "                            #print(\"VALID ARTICLES\", str(valid_articles_count))\n",
    "                            #os.system(command)\n",
    "                  \n",
    "                            \n",
    "    if valid_articles_count == max_articles:\n",
    "        total_articles = index + 1\n",
    "        break\n",
    "print(\"TOTAL\", str(total_articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02d9c0c",
   "metadata": {},
   "source": [
    "Let's check if the output dir contain 100 articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb22130",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_count = len(os.listdir(out_dir))\n",
    "assert article_count==max_articles, 'Invalid number of article(s): {}! Expected number: {}'.format(article_count, max_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55f271d",
   "metadata": {},
   "source": [
    "At the end of this section, we now have a sample including 100 articles that will be the basis of our scientific recommendation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a518f3c4",
   "metadata": {},
   "source": [
    "## 2. Named Entity Recognition (NER) + Named Entity Linking (NEL)\n",
    "\n",
    "**Objective**: To recognize chemical and disease entities in the retrieved articles and to link them to the respective ontology identifiers.\n",
    "\n",
    "We are going to use the [Disease Ontology](https://disease-ontology.org/) (DO), and the [Chemical Entities of Biological Interest](https://www.ebi.ac.uk/chebi/) (ChEBI) ontology.\n",
    "\n",
    "To perform NER and NEL, we are going to apply Minimal Named-Entity Recognizer [MER](https://pypi.org/project/merpy/) tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbb0741",
   "metadata": {},
   "source": [
    "# 2.1. Import libraries\n",
    "<a id='#2.2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe4ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import merpy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc2b517",
   "metadata": {},
   "source": [
    "### 2.2. Configure MER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6607dc",
   "metadata": {},
   "source": [
    "First, we need to download the owl. file associated with ChEBI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fe6281",
   "metadata": {},
   "outputs": [],
   "source": [
    "merpy.download_lexicon(\"ftp://ftp.ebi.ac.uk/pub/databases/chebi/ontology/chebi.owl\",\n",
    "                       \"chebi\", ltype=\"owl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f395d18b",
   "metadata": {},
   "source": [
    "Then, we need to process the downloaded file into a lexicon that MER can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36abd3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merpy.process_lexicon(\"chebi\", ltype=\"owl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a71aa0",
   "metadata": {},
   "source": [
    "We are going to delete obsolete concepts still present in the ontology file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e197a193",
   "metadata": {},
   "outputs": [],
   "source": [
    "merpy.delete_obsolete(\"chebi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66803ec",
   "metadata": {},
   "source": [
    "We need to repeat the operations for the DO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dbc078",
   "metadata": {},
   "outputs": [],
   "source": [
    "merpy.download_lexicon(\"http://purl.obolibrary.org/obo/doid.owl\", \n",
    "                        \"do\", ltype=\"owl\")\n",
    "            \n",
    "merpy.process_lexicon(\"do\", ltype=\"owl\")\n",
    "\n",
    "merpy.delete_obsolete(\"do\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b861c3f7",
   "metadata": {},
   "source": [
    "Let's check the lexicons available for MER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ea09ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "merpy.show_lexicons()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39189eb2",
   "metadata": {},
   "source": [
    "### 2.3. Extract the entities in a single file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aabcfec",
   "metadata": {},
   "source": [
    "Let's retrieve a file from the articles sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c00e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'cord19_small/'\n",
    "\n",
    "with open(dataset_dir + '348055649b6b8cf2b9a376498df9bf41f7123605.json') as article1_file:\n",
    "    article_data = json.load(article1_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b546b138",
   "metadata": {},
   "source": [
    "Let's check the contents of the article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343fb7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd1ccc6",
   "metadata": {},
   "source": [
    "We want to recognize the entities present in title, abstract, and body. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a04f7fa",
   "metadata": {},
   "source": [
    "First, let's retrieve the title, which is a value associated with the key 'metadata':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af6ec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = article_data['metadata']['title']\n",
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd5879c",
   "metadata": {},
   "source": [
    "Then, we apply MER to the title in order recognize disease entities and to link them to DO concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41d3b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "merpy.get_entities(title, 'do')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2538335e",
   "metadata": {},
   "source": [
    "Let's check if the annotations make sense. For instance, access the link http://purl.obolibrary.org/obo/DOID_850."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c888ad63",
   "metadata": {},
   "source": [
    "The entity 'lung disease' in the article was linked to the DO concept 'lung disease' with the ID 'DOID:850', which seems correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1ffd71",
   "metadata": {},
   "source": [
    "Let's apply MER to recognize chemical entities and to link them to ChEBI concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ea500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merpy.get_entities(title, 'chebi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fc75ab",
   "metadata": {},
   "source": [
    "Accessing the link http://purl.obolibrary.org/obo/CHEBI_16480, we can see that the entity 'nitric oxide' was linked to the ChEBI concept 'nitric oxide', which has the ID 'CHEBI:16480'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd3965",
   "metadata": {},
   "source": [
    "We add the disease and chemical entities to a single list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cac3147",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_entities = merpy.get_entities(title, 'do') + merpy.get_entities(title, 'chebi')\n",
    "\n",
    "title_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f327581a",
   "metadata": {},
   "source": [
    "Now, we are going to apply MER to recognize entities in abstract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15605b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = article_data['abstract'][0]['text']\n",
    "\n",
    "abstract_entities = merpy.get_entities(abstract, 'do') + merpy.get_entities(abstract, 'chebi')\n",
    "\n",
    "abstract_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317fbe0d",
   "metadata": {},
   "source": [
    "Let's apply MER in the text associated with the body of the article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e860e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_text = str()\n",
    "\n",
    "for section in article_data['body_text']:\n",
    "    body_text += section['text'] + \"\\n\"\n",
    "\n",
    "body_entities = merpy.get_entities(body_text, \"do\") + merpy.get_entities(body_text, \"chebi\")\n",
    "\n",
    "body_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd675f5b",
   "metadata": {},
   "source": [
    "At last, we need to obtain information about the frequency of each ontology identifier in the document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6057c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_entities = title_entities + abstract_entities + body_entities\n",
    "\n",
    "all_uris = [entity[3] for entity in entities]\n",
    "\n",
    "entity_counter = Counter(all_uris)\n",
    "\n",
    "entity_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959c39d2",
   "metadata": {},
   "source": [
    "To sort the URIs by descending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19745044",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_counter = {\n",
    "    k: v \n",
    "    for k, v in sorted(entity_counter.items(), key=lambda item: item[1], reverse=True)\n",
    "    }\n",
    "\n",
    "entity_counter "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b10d98",
   "metadata": {},
   "source": [
    "### 2.4. Create entity files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f26f082",
   "metadata": {},
   "source": [
    "We need to adapt our code to perform NER and NEL in all documents of our sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb32726",
   "metadata": {},
   "source": [
    "First, create the output dir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2ff5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'cord19_sample_entities/'\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853545b7",
   "metadata": {},
   "source": [
    "Then, initiallize the necessary variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4931c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_dir = 'cord19_small/'\n",
    "\n",
    "output = {'id': str(), 'entities': {}, 'sections': {'title': [], 'abstract': [], 'body': []}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb79da2",
   "metadata": {},
   "source": [
    "Next, we are going to iterate over on each file present in the sample directory, annotate them, and create the respective entity file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a7b6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in os.listdir(article_dir):\n",
    "    \n",
    "    #open the article file\n",
    "    with open(article_dir + article) as input_file:\n",
    "        article_data = json.load(input_file)\n",
    "    \n",
    "    output['id'] = article_data['paper_id']\n",
    "    \n",
    "    # Annotate the title\n",
    "    title = article_data['metadata']['title']\n",
    "    title_entities = merpy.get_entities(title, 'do') + merpy.get_entities(title, 'chebi')\n",
    "    output['sections']['title'] = title_entities\n",
    "    \n",
    "    # Annotate the abstract\n",
    "    abstract = article_data['abstract'][0]['text']\n",
    "    abstract_entities = merpy.get_entities(abstract, 'do') + merpy.get_entities(abstract, 'chebi')\n",
    "    output['sections']['abstract'] = abstract_entities\n",
    "    \n",
    "    # Combine the body text and annotate it\n",
    "    body_text = str()\n",
    "\n",
    "    for section in article_data['body_text']:\n",
    "        body_text += section['text'] + '\\n'\n",
    "\n",
    "    body_entities = merpy.get_entities(body_text, 'do') + merpy.get_entities(body_text, 'chebi')\n",
    "    output['sections']['body'] = body_entities\n",
    "\n",
    "    # Count URIs frequencies and sort them\n",
    "    total_entities = title_entities + abstract_entities + body_entities\n",
    "    all_uris = [entity[3] for entity in total_entities if len(entity)==4]\n",
    "    entity_counter = Counter(all_uris)\n",
    "    \n",
    "    output['entities'] = {\n",
    "        k: v \n",
    "        for k, v in sorted(entity_counter.items(), key=lambda item: item[1], reverse=True)\n",
    "        }\n",
    "\n",
    "    # Generate JSON file with output\n",
    "    out_filepath = out_dir + output['id'] + '_entities.json'\n",
    "    \n",
    "    with open(out_filepath, 'w') as out_file:\n",
    "        out_file.write(json.dumps(output, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a55f11",
   "metadata": {},
   "source": [
    "Now we have both the article files ('covid19_sample' dir) and the respective entities files ('covid19_sample_entities'), and the next step will be the generation of the scientific recomendation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4d72b7",
   "metadata": {},
   "source": [
    "## 3. Creating the recommendation dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys_scifi",
   "language": "python",
   "name": "recsys_scifi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
