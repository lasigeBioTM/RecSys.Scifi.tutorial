{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tutorial: Creating Recommender Systems Datasets in Scientific Fields\n",
    "\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#1\">1. Data retrieval and cleaning</a></li>\n",
    "</ul>\n",
    "   \n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#1.1\">1.1.Import libraries</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#1.2\">1.2. Retrieve CORD-19</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#1.3\">1.3. Exploring the articles of the dataset</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#1.4\">1.4. Selecting a sample of articles to build our scientific recommendation dataset</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li><a href=\"#2\">2. Named Entity Recognition (NER) + Named Entity Linking (NEL)</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#2.1\">2.1. Import libraries</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#2.2\">2.2. Configure MER</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#2.3\">2.3. Import stop words vocabulary and tokenizer</a></li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#2.4\">2.4. Extract the entities in a single file</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#2.5\">2.5. Create entity files</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li><a href=\"#3\">3. Creating the recommendation dataset</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#3.1\">3.1. Import libraries</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#3.2\">3.2. Import user-defined-functions</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#3.3\">3.3. Get entities labels</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#4.4\">3.4. Loading ontologies</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#3.5\">3.5. Save data</a></li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "## 1. Data retrieval + cleaning\n",
    "\n",
    "**Goal**: To retrieve the [COVID-19 Open Research Dataset (CORD-19)](https://www.semanticscholar.org/cord19) and to select a sample of complete English articles (authors' info, title, body text) to build a scientific recommendation dataset.\n",
    "\n",
    "CORD-19 includes coronavirus-related research articles extracted from several sources, such as PubMed, bioRxiv, medRxiv, WHO."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"1.1\"></a>\n",
    "### 1.1 Import libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sys\n",
    "from langdetect import detect\n",
    "sys.path.append(\"./\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"1.2\"></a>\n",
    "### 1.2. Retrieve CORD-19\n",
    "\n",
    "CORD-19 is a large dataset, so in this tutorial we are going to use a smaller version of the dataset. \n",
    "This version is located under the directory \"cord19_small\":"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "os.chdir('data')\n",
    "os.system('tar -xvf cord19_small.tar.xz')\n",
    "os.chdir('../')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, if you want to retrieve the entire dataset, you can run the following code:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "os.chdir('data')\n",
    "         \n",
    "version = 'cord-19_2020-05-12.tar.gz'\n",
    "\n",
    "url = 'https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/' + version\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "os.system('tar -xvf cord-19_2020-05-12.tar.gz')\n",
    "os.chdir('../')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's explore the contents of the dataset directory, particularly, the metadata file."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset_dir = 'data/cord19_small/'\n",
    "metadata_filepath = dataset_dir + 'metadata.csv'\n",
    "\n",
    "metadata = pd.read_csv(metadata_filepath, sep = ',', quotechar = '\"',  encoding = 'utf-8', dtype=str) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have a DataFrame with the contents of the metadata file."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To print column names and first row:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metadata.head(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To access individual rows/articles:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metadata.loc[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To access the individual column title':"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metadata['title']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check the summary statistics:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metadata.count()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "By looking at the statistics, we can see that the number of records with data for the column 'cord_uid' is higher than the number of records with data for the column 'authors'. \n",
    "\n",
    "For our dataset, we only want to include articles with the following characteristics:\n",
    "- authors' information\n",
    "- available title\n",
    "- available body text\n",
    "- article text expressed in English\n",
    "- non-duplicate articles"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"1.3\"></a>\n",
    "\n",
    "### 1.3. Exploring the articles of the dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's consider the first article appearing in the metadata file.\n",
    "To check if information about the article's author is available:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metadata.loc[0]['authors']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To check if there is an available title:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metadata.loc[0]['title']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the title is expressed in English, but it would not be efficient to check the language of every article in the dataset, so we will apply the language detection tool [langdetect](https://pypi.org/project/langdetect/):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "title1 = metadata.loc[0]['title']\n",
    "\n",
    "title1_lang = (title1)\n",
    "\n",
    "print(\"Title:\", title1, \"\\nLanguage:\", title1_lang)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The tool detects English as the language of the title.\n",
    "\n",
    "Let's check another article:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "title2 = metadata.loc[144]['title']\n",
    "\n",
    "title2_lang = detect(title2)\n",
    "\n",
    "print(\"Title:\", title2, \"\\nLanguage:\", title2_lang)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this case, the tool detects german ('de') as the language of the title."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we want to check if there is an available abstract for the first article:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metadata.loc[0]['abstract']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The metadata file does not contain the article's text besides abstract and title, but we can access the file associated with the article using the provided information in the columns 'pdf_json_files' or 'pmc_json_files':"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metadata.loc[0]['pdf_json_files']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's open the file, which is in [JSON](https://www.json.org/json-en.html) format:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "article1_filepath = dataset_dir + metadata.loc[0]['pdf_json_files']\n",
    "\n",
    "with open(article1_filepath, encoding='utf-8') as article1_file:\n",
    "    article1_data = json.load(article1_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have the file content stored in a dictionary with the following keys:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "article1_data.keys()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The article content is the following:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "article1_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check the body text:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "body_text = article1_data['body_text']\n",
    "body_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have a list of dictionaries, each dictionary is a paragraph belonging to a given section of the article. We want to join the different parts text in a single string: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "article1_text = str()\n",
    "\n",
    "for paragraph in body_text:\n",
    "    article1_text += paragraph['text'] + '\\n'\n",
    "\n",
    "article1_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "All good! We will include this article in our dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"1.4\"></a>\n",
    "\n",
    "### 1.4. Selecting a sample of articles to build our scientific recommendation dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instead of repating each operation for each file individually, let us adapt our code to automatically select a sample containing 100 preprocessed articles. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, create the output directory:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "out_dir = 'data/sample/'\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, initiallize the necessary variables:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "max_articles = 100 #number of articles to include in the sample\n",
    "dataset_dir = 'data/cord19_small/'\n",
    "metadata_filepath = dataset_dir + 'metadata.csv'\n",
    "valid_articles_count = int()\n",
    "out_articles_ids = list()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Open the metadata file:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metadata = pd.read_csv(metadata_filepath, sep = ',', quotechar = '\"',  encoding = 'utf-8', dtype=str) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then iterate over the records in the metadata file and choose only the relevant ones:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "valid_articles_count = int()\n",
    "blacklist = str()\n",
    "blacklist_count = int()\n",
    "\n",
    "for index, record in metadata.iterrows():\n",
    "    invalid_article = True\n",
    "    \n",
    "    if valid_articles_count <= max_articles:\n",
    "        \n",
    "        if record['pubmed_id'] not in out_articles_ids:\n",
    "            \n",
    "            if type(record['sha']) != float:\n",
    "\n",
    "                if record['authors'] != '':    \n",
    "\n",
    "                    if record['title'] != '':\n",
    "                        title = record['title']\n",
    "                        title_lang = detect(title)\n",
    "                        article_filepath = record['pdf_json_files']\n",
    "                        \n",
    "                        if article_filepath != '': # to consider onyl articles from the pdf_json directory\n",
    "\n",
    "                            if title_lang == 'en'  \\\n",
    "                                and type(article_filepath) != float  \\\n",
    "                                and article_filepath.count(\"document\") == 1:\n",
    "\n",
    "                                article_filepath_up = dataset_dir + record['pdf_json_files']\n",
    "\n",
    "                                with open(article_filepath_up, encoding='utf-8') as article_file:\n",
    "                                    article_data = json.load(article_file)\n",
    "\n",
    "                                if 'body_text' in article_data.keys(): # the article is valid\n",
    "                                    valid_articles_count += 1\n",
    "                                    invalid_article = False\n",
    "\n",
    "                                    # open the article file to check if it contains all info\n",
    "                                    with open(article_filepath_up) as article_file:\n",
    "                                        article_data = json.load(article_file)\n",
    "\n",
    "                                    # correct the info of the article with info present in metadata file\n",
    "                                    changed_article = False\n",
    "\n",
    "                                    if article_data['metadata']['title'] == '':\n",
    "                                        article_data['metadata']['title'] = record['title']\n",
    "                                        changed_article = True\n",
    "\n",
    "                                    if article_data['metadata']['authors'] == []:\n",
    "                                        article_data['metadata']['authors'] = record['authors']\n",
    "\n",
    "                                    # output or copy article file to out_dir\n",
    "                                    if changed_article:\n",
    "\n",
    "                                        with open(out_dir + record['sha'] + '.json', 'w') as out_file:\n",
    "                                            out_file.write(json.dumps(article_data, indent=4, ensure_ascii=False))\n",
    "\n",
    "                                    else:\n",
    "                                        command = 'cp '  \\\n",
    "                                                  + article_filepath_up + ' ' \\\n",
    "                                                  + out_dir  \\\n",
    "                                                  + record['sha'] + '.json'\n",
    "\n",
    "                                        os.system(command)\n",
    "        \n",
    "        if invalid_article: # store article pubmed id in blacklist file\n",
    "            blacklist += record['pubmed_id'] + \"\\n\"\n",
    "            blacklist_count += 1\n",
    "            \n",
    "    if valid_articles_count == max_articles:\n",
    "        total_articles = index + 1\n",
    "        break\n",
    "\n",
    "#Create blacklist file with info about invalid articles\n",
    "with open('data/blacklist/blacklist_articles.txt', 'w') as blacklist_file:\n",
    "    blacklist_file.write(blacklist)\n",
    "    blacklist_file.close()\n",
    "\n",
    "print(\"Invalid articles:\", str(blacklist_count))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you were not able to run the code, you can uncompress the file 'sample.tar.xz' under 'data' directory."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check if the output directory contain the desired number of articles (max_articles):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "article_count = len(os.listdir(out_dir))\n",
    "assert article_count == max_articles, 'Invalid number of article(s): {}! Expected number: {}'.format(article_count, max_articles)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "At the end of this section, we now have a sample including 100 articles that will be the basis of our scientific recommendation dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2. Named Entity Recognition (NER) + Named Entity Linking (NEL)\n",
    "\n",
    "**Goal**: To recognize chemical and disease entities in the retrieved articles and to link them to the respective ontology identifiers.\n",
    "\n",
    "We are going to use the [Disease Ontology](https://disease-ontology.org/) (DO), and the [Chemical Entities of Biological Interest](https://www.ebi.ac.uk/chebi/) (ChEBI) ontology.\n",
    "\n",
    "To perform NER and NEL, we are going to apply Minimal Named-Entity Recognizer [MER](https://pypi.org/project/merpy/) tool."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"2.1\"></a>\n",
    "### 2.1. Import libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import merpy\n",
    "import multiprocessing\n",
    "from collections import Counter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"2.2\"></a>\n",
    "### 2.2. Configure MER"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we need to download the owl file associated with ChEBI:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "merpy.download_lexicon(\"ftp://ftp.ebi.ac.uk/pub/databases/chebi/ontology/chebi.owl\",\n",
    "                       \"chebi\", ltype=\"owl\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we need to process the downloaded file into a lexicon that MER can use:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "merpy.process_lexicon(\"chebi\", ltype=\"owl\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are going to delete obsolete concepts still present in the ontology file:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "merpy.delete_obsolete(\"chebi\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to repeat the operations for the DO:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "merpy.download_lexicon(\"http://purl.obolibrary.org/obo/doid.owl\", \n",
    "                        \"do\", ltype=\"owl\")\n",
    "            \n",
    "merpy.process_lexicon(\"do\", ltype=\"owl\")\n",
    "\n",
    "merpy.delete_obsolete(\"do\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check the lexicons available for MER:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "merpy.show_lexicons()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"2.3\"></a>\n",
    "### 2.3. Import stop words vocabulary and tokenizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stop words are common words of a given language (for example the words 'the', 'and', 'in'). A typical pre-processing step is to tokenize the text and remove the stopwords. For that, we are going to import NLTK's list of english stopwords and use the NLTK tokenizer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import nltk \n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "all_stopwords = stopwords.words('english')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are going to extend the stop words vocabulary by adding stop words associated with ChEBI and DO:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "kbs_stopwords = list()\n",
    "blacklist_dir = 'data/blacklist/'\n",
    "filenames = ['chebi.txt', 'doid.txt']\n",
    "\n",
    "for filename in filenames:\n",
    "    \n",
    "    with open(blacklist_dir + filename, 'r') as backlist_file:\n",
    "        stopwords = [content.strip('\\n') for content in backlist_file.readlines()]\n",
    "        kbs_stopwords.extend(stopwords)\n",
    "        backlist_file.close()\n",
    "\n",
    "#Extend stop words vocabulary with the retrieved KBs stopwords\n",
    "all_stopwords.extend(kbs_stopwords)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"2.4\"></a>\n",
    "### 2.4. Extract the entities in a single file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's retrieve a file from the articles sample:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset_dir = 'data/sample/'\n",
    "\n",
    "with open(dataset_dir + '87390d2ae28407b3e03e60a6b24a7fd99ed7229a.json') as article1_file:\n",
    "    article_data = json.load(article1_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check the contents of the article:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "article_data.keys()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to recognize the entities present in title, abstract, and body. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, let's retrieve the title, which is a value associated with the key 'metadata':"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "title = article_data['metadata']['title']\n",
    "title"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's tokenize the title:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "title_tokens = word_tokenize(title)\n",
    "title_tokens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And remove the tokens relative to stop words:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "title_tokens_up = [word for word in title_tokens if not word in all_stopwords]\n",
    "title_tokens_up"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And rebuild the title without the stop words:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "title_up = (' ').join(title_tokens_up)\n",
    "title_up"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we apply MER to the preprocessed title in order to recognize disease entities and to link them to DO concepts:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "merpy.get_entities(title_up, 'do')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check if the annotations make sense. For instance, access the link http://purl.obolibrary.org/obo/DOID_2945."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The entity 'SARS' in the article was linked to the DO concept 'severe acute respiratory syndrome' with the identifier 'DOID:2945', which seems correct!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's apply MER to recognize chemical entities and to link them to ChEBI concepts:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "merpy.get_entities(title_up, 'chebi')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Accessing the link http://purl.obolibrary.org/obo/CHEBI_35341, we can see that the entity 'Steroids' was linked to the ChEBI concept 'steroid', which has the identifier 'CHEBI:35341'."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We add the disease and chemical entities to a single list:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "title_entities = merpy.get_entities(title_up, 'do') + merpy.get_entities(title_up, 'chebi')\n",
    "\n",
    "title_entities"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we are going to apply MER to recognize entities in abstract:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "abstract = article_data['abstract'][0]['text']\n",
    "\n",
    "#Tokenize and remove stop words\n",
    "abstract_tokens = word_tokenize(abstract)\n",
    "abstract_tokens_up = [word for word in abstract_tokens if not word in all_stopwords]\n",
    "abstract_up = (' ').join(abstract_tokens_up)\n",
    "\n",
    "#Entity recognition and linking \n",
    "abstract_entities = merpy.get_entities(abstract_up, 'do') + merpy.get_entities(abstract_up, 'chebi')\n",
    "\n",
    "abstract_entities"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's apply MER in the text associated with the body of the article:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "body = str()\n",
    "\n",
    "for section in article_data['body_text']:\n",
    "    body += section['text'] + \"\\n\"\n",
    "    \n",
    "#Tokenize and remove stop words\n",
    "body_tokens = word_tokenize(body)\n",
    "body_tokens_up = [word for word in body_tokens if not word in all_stopwords]\n",
    "body_up = (' ').join(body_tokens_up)\n",
    "\n",
    "#Entity recognition and linking \n",
    "body_entities = merpy.get_entities(body_up, \"do\") + merpy.get_entities(body_up, \"chebi\")\n",
    "\n",
    "body_entities"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "At last, we need to obtain information about the frequency of each ontology identifier in the document:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "total_entities = title_entities + abstract_entities + body_entities\n",
    "\n",
    "all_uris = [entity[3] for entity in total_entities]\n",
    "\n",
    "entity_counter = Counter(all_uris)\n",
    "\n",
    "entity_counter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To sort the URIs by descending order:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "entity_counter = {\n",
    "    k: v \n",
    "    for k, v in sorted(entity_counter.items(), key=lambda item: item[1], reverse=True)\n",
    "    }\n",
    "\n",
    "entity_counter "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"2.5\"></a>\n",
    "### 2.5. Create entity files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to adapt our code to perform NER and NEL in all documents of our sample."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, create the output dir:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "out_dir = 'data/sample_entities/'\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we are going to iterate over each file present in the sample directory, annotate it, and create the respective entity file:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def annotate_doc(article):\n",
    "    \n",
    "    article_filepath = 'data/sample/' + article  \n",
    "\n",
    "    with open(article_filepath) as input_file:\n",
    "        article_data = json.load(input_file)\n",
    "\n",
    "    doc_output = {'id': str(), 'entities': {}, 'sections': {'title': [], 'abstract': [], 'body': []}}\n",
    "    \n",
    "    doc_output['id'] = article_data['paper_id']\n",
    "   \n",
    "    #Annotate the title\n",
    "    title = article_data['metadata']['title']\n",
    "    \n",
    "    #Tokenize and remove stop words\n",
    "    title_tokens = word_tokenize(title)\n",
    "    title_tokens_up = [word for word in title_tokens if not word in all_stopwords]\n",
    "    title_up = (' ').join(title_tokens_up)\n",
    "    \n",
    "    #Entity recognition and linking\n",
    "    title_entities = merpy.get_entities(title_up, 'do') + merpy.get_entities(title_up, 'chebi')\n",
    "    doc_output['sections']['title'] = title_entities\n",
    "    \n",
    "    #Annotate the abstract\n",
    "    if article_data['abstract'] != []:\n",
    "        abstract = article_data['abstract'][0]['text']\n",
    "        \n",
    "        #Tokenize and remove stop words\n",
    "        abstract_tokens = word_tokenize(abstract)\n",
    "        abstract_tokens_up = [word for word in abstract_tokens if not word in all_stopwords]\n",
    "        abstract_up = (' ').join(abstract_tokens_up)\n",
    "        \n",
    "        #Entity recognition and linking\n",
    "        abstract_entities = merpy.get_entities(abstract_up, 'do') + merpy.get_entities(abstract_up, 'chebi')\n",
    "        doc_output['sections']['abstract'] = abstract_entities\n",
    "    \n",
    "    else:\n",
    "        abstract_entities = []\n",
    "\n",
    "    #Combine the several paragraphs of the body text and annotate it\n",
    "    body = str()\n",
    "\n",
    "    for section in article_data['body_text']:\n",
    "        body += section['text'] + '\\n'\n",
    "    \n",
    "    #Tokenize and remove stop words\n",
    "    body_tokens = word_tokenize(body)\n",
    "    body_tokens_up = [word for word in body_tokens if not word in all_stopwords]\n",
    "    body_up = (' ').join(body_tokens_up)    \n",
    "        \n",
    "    #Entity recognition and linking\n",
    "    body_entities = merpy.get_entities(body_up, 'do') + merpy.get_entities(body_up, 'chebi')\n",
    "    doc_output['sections']['body'] = body_entities\n",
    "\n",
    "    # Count URIs frequencies and sort them\n",
    "    total_entities = title_entities + abstract_entities + body_entities\n",
    "    all_uris = [entity[3] for entity in total_entities if len(entity)==4]\n",
    "    entity_counter = Counter(all_uris)\n",
    "    \n",
    "    doc_output['entities'] = {\n",
    "        k: v \n",
    "        for k, v in sorted(entity_counter.items(), key=lambda item: item[1], reverse=True)\n",
    "        }\n",
    "    \n",
    "    #Generate JSON file with output\n",
    "    out_filepath = out_dir + doc_output['id'] + '_entities.json'\n",
    "    \n",
    "    with open(out_filepath, 'w') as out_file:\n",
    "        out_file.write(json.dumps(doc_output, indent=4))\n",
    "        \n",
    "\n",
    "article_dir = 'data/sample/'\n",
    "        \n",
    "with multiprocessing.Pool(processes=10) as pool: \n",
    "    # change the number of processes according to number of available cores\n",
    "    outputs = pool.map(annotate_doc, [article for article in os.listdir(article_dir)], chunksize=10)\n",
    "    pool.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you were not able to run the previous code, you can extract the file 'sample_entities.tar.xz' under 'data' directory"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have both the article files ('data/sample' directory) and the respective entities files ('data/sample_entities' directory), and the next step will be the generation of the scientific recomendation dataset using the LIBRETTI algorithm."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. Create the recommendation dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"3.1\"></a>\n",
    "### 3.1. Import libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import date, datetime"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"3.2\"></a>\n",
    "### 3.2. Import user-defined-functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from util.utils import *\n",
    "from util.utils2json import *\n",
    "from util.functions import *\n",
    "from util.utils2ontologies import *\n",
    "from util.utils2pubmed import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"max_rows\", None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Entities defined by user, our example will work with chebi and doid ontologies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# if entities is defined by user then saved it in a list\n",
    "active_lexicons = []\n",
    "is_chebi, is_do, is_go, is_hp = True, True, False, False\n",
    "\n",
    "active_lexicons.append('chebi')   \n",
    "active_lexicons.append('do')   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Path of original' json and entities' json folder, blacklist and metadata"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "original_json_folder = '../data/sample/'\n",
    "entities_json_folder = '../data/sample_entities/'\n",
    "path_to_blacklist = '../data/blacklist/blacklist_articles.txt'\n",
    "path_to_metadata = '../data/metadata.csv'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "entities_list_of_json_files = list_files_in_directory(path=entities_json_folder)\n",
    "#print(entities_list_of_json_files)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "##### Get all articles id that cannot be considered in use case "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "articles_blacklist = get_blacklist(file=path_to_blacklist)\n",
    "print(articles_blacklist)\n",
    "metadata = pd.read_csv(path_to_metadata)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "user_item_rating_all = []\n",
    "count = 0\n",
    "#entities_list_of_json_files=['PMC7176216_entities.json']\n",
    "for file in entities_list_of_json_files:\n",
    "    # ## Exception\n",
    "    if file.replace('_entities.json','') in articles_blacklist:\n",
    "        continue\n",
    "    # if file.startswith('PMC'):\n",
    "    #     continue    \n",
    "    print(count, \"-\", len(entities_list_of_json_files))            \n",
    "    print(file)\n",
    "        \n",
    "    # check valid json file, i.e. contains values\n",
    "    try:\n",
    "        j_file_entities = open_json_file_pd(path=entities_json_folder, file=file)           \n",
    "    except Exception as e:\n",
    "        print(f'Json file does not contain values. Error message {e}')\n",
    "        set_blacklist(file=path_to_blacklist, line=file.replace('_entities.json',''))\n",
    "        continue\n",
    "    \n",
    "    df_entities = get_entities_id(get_entities(j_file_entities))\n",
    "    if df_entities.empty:\n",
    "        print(f'Json file does not contain values.')\n",
    "        set_blacklist(file=path_to_blacklist, line=file.replace('_entities.json',''))\n",
    "        continue    \n",
    "    \n",
    "    print(df_entities)\n",
    "    article_id = get_article_id(j_file_entities)\n",
    "    \n",
    "    # check valid json file, i.e. contains values\n",
    "    try:\n",
    "        j_file_original = open_json_file(path=original_json_folder, file=article_id)\n",
    "        #print(j_file_original)\n",
    "    except Exception as e:\n",
    "        print(f'Original json file does not exist. Error message {e}')\n",
    "        set_blacklist(file=path_to_blacklist, line=file.replace('_entities.json',''))\n",
    "        continue    \n",
    "    \n",
    "    # check if json file contains authors, otherwise try to find them in metadata.csv\n",
    "    # if value remains null them put this article in the blacklist file\n",
    "    list_of_authors = get_authors_names(data=j_file_original)\n",
    "    \n",
    "###  ---- Metadata ----\n",
    "    # Some datas return an array, then we convert to a list and get first element\n",
    "    publish_date=''      \n",
    "    if article_id.startswith('PMC'):\n",
    "        try:\n",
    "            publish_date = metadata[metadata.pmcid == article_id].\\\n",
    "                publish_time.map(lambda v: v.split('-')[0]).tolist()[0]\n",
    "        except Exception as e:    \n",
    "            try:\n",
    "                print(f'Find publish date, now with metapub. Error message {e}')            \n",
    "                publish_date = get_year_by_metapub(pmcid=article_id) \n",
    "            except Exception as e: \n",
    "                try:                    \n",
    "                    print(f'Find publish date, now with Bio. Error message {e}') \n",
    "                    pmid = get_pmid(pmcid=article_id) \n",
    "                    publish_date = get_year_by_bio(pmid) \n",
    "                except Exception as e:\n",
    "                    print(f'Find publish date, now with metapub. Error message {e}')    \n",
    "                \n",
    "        finally:\n",
    "            pass                                         \n",
    "    else:\n",
    "        try:\n",
    "            publish_date = metadata[metadata.sha == article_id].\\\n",
    "                publish_time.map(lambda v: v.split('-')[0]).tolist()[0]\n",
    "        finally:\n",
    "            pass  \n",
    "\n",
    "    print(publish_date)\n",
    "    if publish_date==None:\n",
    "        set_blacklist(file=path_to_blacklist, line=file.replace('_entities.json',''))\n",
    "        continue\n",
    "    \n",
    "    # if authors is empty we will find in metadata.csv file\n",
    "    if len(list_of_authors)==0:\n",
    "        ##if string is NaN\n",
    "        try:\n",
    "            if article_id.startswith('PMC'):\n",
    "                try:\n",
    "                    authors = get_authors_metadata(data=metadata, label=pmcid, ident= article_id)\n",
    "                    list_of_authors.append(authors) \n",
    "                except Exception as e:\n",
    "                    print(f'Get authors name in pubmed. Error: {e}')    \n",
    "                    pmid = get_pmid(pmcid=article_id)\n",
    "                    authors = get_authors_by_bio(pmid)\n",
    "                    for a in authors:                \n",
    "                        first = unidecode.unidecode(a['first'])\n",
    "                        last = unidecode.unidecode(a['last']) \n",
    "                    list_of_authors.append(first + ', '+  last)                     \n",
    "            else:\n",
    "                authors = get_authors_metadata(data=metadata, label=sha, ident= article_id)\n",
    "                list_of_authors.append(authors) \n",
    "        except Exception as e:\n",
    "            print(f'Empty values {e}')   \n",
    "            set_blacklist(file=path_to_blacklist, line=file.replace('_entities.json',''))                \n",
    "        continue\n",
    "    count+=1\n",
    "    \n",
    "###  ---- End of Metadata ----            \n",
    "    \n",
    "    user_item_rating = get_user_item_rating(lst=list_of_authors, df=df_entities)\n",
    "    #print(user_item_rating)\n",
    "    \n",
    "    ## add publish_date in array in index column = 3\n",
    "    user_item_rating = np.insert(user_item_rating, 3, publish_date, axis=1)\n",
    "    # print(user_item_rating)\n",
    "    user_item_rating_all.append(user_item_rating)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "flat_list = []\n",
    "for sublist in user_item_rating_all:\n",
    "    for item in sublist:\n",
    "        flat_list.append(item)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "array = np.array(flat_list)\n",
    "\n",
    "final_data = pd.DataFrame(array,  columns=['user', 'item', 'rating', 'year'])\n",
    "sum_df = final_data.groupby(['user', 'item', 'year']).size().reset_index().rename(columns={0: 'rating'})    \n",
    "df_with_user_id = id_to_index(sum_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# swap columns: user and index_user, and after rename to author_name\n",
    "#df_with_user_id['index_user'], df_with_user_id['user'] = df_with_user_id['user'], df_with_user_id['index_user']\n",
    "df_with_user_id.rename(columns={'user': 'author_name', 'index_user': 'user'}, inplace = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"3.3\"></a>\n",
    "### 3.3. Get entities labels"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "list_of_entities = df_with_user_id.item.unique()\n",
    "print(list_of_entities) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"3.4\"></a>\n",
    "### 3.4. Loading ontologies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "chebi, do, go, hp = loading_items(is_chebi, is_do, is_go, is_hp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "entities_label = get_entities_labels(lst=list_of_entities, prefix_chebi=chebi, prefix_do=do, prefix_go=go, prefix_hp=hp)\n",
    "print(entities_label)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_entities = pd.DataFrame(list_of_entities, columns=[\"item_id\"])\n",
    "df_entities[\"entity_name\"] = np.array(entities_label)\n",
    "print(df_entities)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('mapping labels')\n",
    "df_with_user_id[\"item_name\"] = df_with_user_id[\"item\"].map(df_entities.set_index('item_id')[\"entity_name\"]).fillna(0)\n",
    "print(df_with_user_id)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"3.5\"></a>\n",
    "### 3.5. Save data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "path_to_cord_ds = '../data/results/sample_cord-19_dataset.csv'\n",
    "path_to_cord_userid = '../data/results/sample_cord-19_dataset_userid.csv'\n",
    "print('saving data')\n",
    "save_final_data(data=df_with_user_id[['user', 'item', 'rating', 'item_name', 'year']], \\\n",
    "        path=path_to_cord_ds)\n",
    "save_final_data(data=df_with_user_id[['user', 'author_name']], \\\n",
    "        path=path_to_cord_userid)\n",
    "#save_final_data(data=df_with_user_id[['user', 'author_name', 'item', 'rating', 'item_name', 'year']], \\\n",
    "#        path=path_to_cord_userid)  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.0. Data statistics "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('data/results/sample_cord-19_dataset.csv', names=['user', 'item', 'rating', 'item_name', 'year'])\n",
    "#dataset = pd.read_csv('/home/mbarros/Desktop/data/scirec2021/comm_subset_cord-19_dataset_total_ordered_filtered.csv', names=['user', 'item', 'rating', 'item_name', 'year'])\n",
    "\n",
    "\n",
    "print(dataset.head(20))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    user          item  rating  \\\n",
      "0      0   CHEBI_10545       1   \n",
      "1      0  CHEBI_138164       1   \n",
      "2      0  CHEBI_143889       1   \n",
      "3      0   CHEBI_16236       1   \n",
      "4      0   CHEBI_16541       1   \n",
      "5      0   CHEBI_16670       1   \n",
      "6      0   CHEBI_16991       1   \n",
      "7      0   CHEBI_17234       1   \n",
      "8      0   CHEBI_17790       1   \n",
      "9      0   CHEBI_17905       1   \n",
      "10     0   CHEBI_17992       1   \n",
      "11     0   CHEBI_22063       1   \n",
      "12     0   CHEBI_23528       1   \n",
      "13     0   CHEBI_23888       1   \n",
      "14     0   CHEBI_27641       1   \n",
      "15     0   CHEBI_28077       1   \n",
      "16     0   CHEBI_28262       1   \n",
      "17     0   CHEBI_29234       1   \n",
      "18     0   CHEBI_30740       1   \n",
      "19     0   CHEBI_31962       1   \n",
      "\n",
      "                                            item_name  year  \n",
      "0                                            electron  2000  \n",
      "1                                    fomesafen-sodium  2000  \n",
      "2                               galactosaminogalactan  2000  \n",
      "3                                             ethanol  2000  \n",
      "4                           protein polypeptide chain  2000  \n",
      "5                                             peptide  2000  \n",
      "6                               deoxyribonucleic acid  2000  \n",
      "7                                             glucose  2000  \n",
      "8                                            methanol  2000  \n",
      "9                                          coenzyme M  2000  \n",
      "10                                            sucrose  2000  \n",
      "11                                          sulfoxide  2000  \n",
      "12                                       cytochalasin  2000  \n",
      "13                                               drug  2000  \n",
      "14                                      cycloheximide  2000  \n",
      "15                                         rifampicin  2000  \n",
      "16                                 dimethyl sulfoxide  2000  \n",
      "17                                             triton  2000  \n",
      "18  ethylene glycol bis(2-aminoethyl)tetraacetic acid  2000  \n",
      "19                     paraformaldehyde macromolecule  2000  \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# number of unique users\n",
    "# number of unique items\n",
    "# number of ratings\n",
    "# sparsity \n",
    "\n",
    "print('n users: ', dataset.user.unique().shape[0])\n",
    "print('n items: ', dataset.item.unique().shape[0])\n",
    "print('n ratings: ', dataset.size)\n",
    "\n",
    "print('sparsity: ', 1 - (dataset.size / (dataset.user.unique().shape[0] * dataset.item.unique().shape[0])))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "n users:  464\n",
      "n items:  918\n",
      "n ratings:  92065\n",
      "sparsity:  0.7838606227931786\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# items by ontology\n",
    "\n",
    "print(\"CHEBI items: \", dataset[dataset.item.str.startswith('CHEBI')].item.unique().shape[0])\n",
    "\n",
    "print(\"DOID items: \", dataset[dataset.item.str.startswith('DOID')].item.unique().shape[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CHEBI items:  711\n",
      "DOID items:  207\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# items by user\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "unique_users = dataset.user.unique()\n",
    "items_by_user = dataset.groupby(['user'])[\"item\"].count().reset_index()\n",
    "items_by_user = items_by_user.sort_values(by=['item'], ascending=False)\n",
    "print(items_by_user)\n",
    "items_by_user.user = items_by_user.user.astype('str')\n",
    "\n",
    "print('max items by user: ', items_by_user.item.max())\n",
    "print('min items by user: ', items_by_user.item.min())\n",
    "print('mean items by user: ', items_by_user.item.mean())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     user  item\n",
      "348   348   175\n",
      "146   146   140\n",
      "85     85   109\n",
      "29     29   109\n",
      "381   381   109\n",
      "..    ...   ...\n",
      "177   177    13\n",
      "372   372    12\n",
      "88     88    10\n",
      "181   181    10\n",
      "140   140    10\n",
      "\n",
      "[464 rows x 2 columns]\n",
      "max items by user:  175\n",
      "min items by user:  10\n",
      "mean items by user:  39.68318965517241\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.scatter(items_by_user.user, items_by_user.item)\n",
    "plt.axhline(y=items_by_user.item.mean(), color='r', linestyle='-')\n",
    "plt.ylabel('number of items')\n",
    "plt.xlabel('user')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# users by item\n",
    "\n",
    "unique_items = dataset.item.unique()\n",
    "users_by_item = dataset.groupby(['item'])[\"user\"].count().reset_index()\n",
    "users_by_item = users_by_item.sort_values(by=['user'], ascending=False)\n",
    "\n",
    "print('max users by item: ', users_by_item.user.max())\n",
    "list_of_max_items = users_by_item[users_by_item.user == users_by_item.user.max()].item.values\n",
    "print(list_of_max_items)\n",
    "\n",
    "print(dataset[dataset.item == list_of_max_items[0]])\n",
    "\n",
    "print('min users by item: ', users_by_item.user.min())\n",
    "print('mean users by item: ', users_by_item.user.mean())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "max users by item:  460\n",
      "['CHEBI_58972']\n",
      "       user         item  rating            item_name  year\n",
      "36        0  CHEBI_58972       1  (E)-4-oxonon-2-enal  2000\n",
      "79        1  CHEBI_58972       1  (E)-4-oxonon-2-enal  2005\n",
      "134       2  CHEBI_58972       1  (E)-4-oxonon-2-enal  2002\n",
      "173       3  CHEBI_58972       1  (E)-4-oxonon-2-enal  2005\n",
      "194       4  CHEBI_58972       1  (E)-4-oxonon-2-enal  2005\n",
      "...     ...          ...     ...                  ...   ...\n",
      "18224   458  CHEBI_58972       1  (E)-4-oxonon-2-enal  2005\n",
      "18244   459  CHEBI_58972       1  (E)-4-oxonon-2-enal  2003\n",
      "18294   460  CHEBI_58972       1  (E)-4-oxonon-2-enal  2001\n",
      "18339   461  CHEBI_58972       1  (E)-4-oxonon-2-enal  2005\n",
      "18406   463  CHEBI_58972       1  (E)-4-oxonon-2-enal  2004\n",
      "\n",
      "[460 rows x 5 columns]\n",
      "min users by item:  1\n",
      "mean users by item:  20.05773420479303\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "dataset_doid = dataset[dataset.item.str.startswith('DOID')]\n",
    "unique_items_d = dataset_doid.item.unique()\n",
    "users_by_item_d = dataset_doid.groupby(['item'])[\"user\"].count().reset_index()\n",
    "users_by_item_d = users_by_item_d.sort_values(by=['user'], ascending=False)\n",
    "\n",
    "print('max users by item: ', users_by_item_d.user.max())\n",
    "list_of_max_items_d = users_by_item_d[users_by_item_d.user == users_by_item_d.user.max()].item.values\n",
    "print(list_of_max_items_d)\n",
    "\n",
    "print(dataset_doid[dataset_doid.item == list_of_max_items_d[0]])\n",
    "\n",
    "print('min users by item: ', users_by_item_d.user.min())\n",
    "print('mean users by item: ', users_by_item_d.user.mean())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "max users by item:  292\n",
      "['DOID_2945']\n",
      "       user       item  rating                          item_name  year\n",
      "202       4  DOID_2945       1  severe acute respiratory syndrome  2005\n",
      "242       5  DOID_2945       1  severe acute respiratory syndrome  2004\n",
      "243       5  DOID_2945       1  severe acute respiratory syndrome  2005\n",
      "331       8  DOID_2945       1  severe acute respiratory syndrome  2005\n",
      "479      12  DOID_2945       1  severe acute respiratory syndrome  2005\n",
      "...     ...        ...     ...                                ...   ...\n",
      "18162   456  DOID_2945       1  severe acute respiratory syndrome  2004\n",
      "18202   457  DOID_2945       1  severe acute respiratory syndrome  2005\n",
      "18259   459  DOID_2945       1  severe acute respiratory syndrome  2003\n",
      "18345   461  DOID_2945       1  severe acute respiratory syndrome  2005\n",
      "18363   462  DOID_2945       1  severe acute respiratory syndrome  2003\n",
      "\n",
      "[292 rows x 5 columns]\n",
      "min users by item:  1\n",
      "mean users by item:  16.594202898550726\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "dataset_chebi = dataset[dataset.item.str.startswith('CHEBI')]\n",
    "unique_items_c = dataset_chebi.item.unique()\n",
    "users_by_item_c = dataset_chebi.groupby(['item'])[\"user\"].count().reset_index()\n",
    "users_by_item_c = users_by_item_c.sort_values(by=['user'], ascending=False)\n",
    "\n",
    "print('max users by item: ', users_by_item_c.user.max())\n",
    "list_of_max_items_c = users_by_item_c[users_by_item_c.user == users_by_item_c.user.max()].item.values\n",
    "print(list_of_max_items_c)\n",
    "\n",
    "print(dataset_chebi[dataset_chebi.item == list_of_max_items_c[0]])\n",
    "\n",
    "print('min users by item: ', users_by_item_c.user.min())\n",
    "print('mean users by item: ', users_by_item_c.user.mean())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "max users by item:  460\n",
      "['CHEBI_58972']\n",
      "       user         item  rating            item_name  year\n",
      "36        0  CHEBI_58972       1  (E)-4-oxonon-2-enal  2000\n",
      "79        1  CHEBI_58972       1  (E)-4-oxonon-2-enal  2005\n",
      "134       2  CHEBI_58972       1  (E)-4-oxonon-2-enal  2002\n",
      "173       3  CHEBI_58972       1  (E)-4-oxonon-2-enal  2005\n",
      "194       4  CHEBI_58972       1  (E)-4-oxonon-2-enal  2005\n",
      "...     ...          ...     ...                  ...   ...\n",
      "18224   458  CHEBI_58972       1  (E)-4-oxonon-2-enal  2005\n",
      "18244   459  CHEBI_58972       1  (E)-4-oxonon-2-enal  2003\n",
      "18294   460  CHEBI_58972       1  (E)-4-oxonon-2-enal  2001\n",
      "18339   461  CHEBI_58972       1  (E)-4-oxonon-2-enal  2005\n",
      "18406   463  CHEBI_58972       1  (E)-4-oxonon-2-enal  2004\n",
      "\n",
      "[460 rows x 5 columns]\n",
      "min users by item:  1\n",
      "mean users by item:  21.066104078762308\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#%matplotlib qt\n",
    "\n",
    "plt.scatter(users_by_item.item, users_by_item.user)\n",
    "plt.axhline(y=users_by_item.user.mean(), color='r', linestyle='-')\n",
    "#plt.yscale('log')\n",
    "plt.ylabel('number of users')\n",
    "plt.xlabel('items')\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys_scifi",
   "language": "python",
   "name": "recsys_scifi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}