{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tutorial: Creating Recommender Systems Datasets in Scientific Fields\n",
    "\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#1\">1. Data retrieval and cleaning</a></li>\n",
    "</ul>\n",
    "   \n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#1.1\">1.1.Import libraries</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#1.2\">1.2. Retrieve CORD-19</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#1.3\">1.3. Exploring the articles of the dataset</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#1.4\">1.4. Selecting a sample of articles to build our scientific recommendation dataset</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li><a href=\"#2\">2. Named Entity Recognition (NER) + Named Entity Linking (NEL)</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#2.1\">2.1. Import libraries</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#2.2\">2.2. Configure MER</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#2.3\">2.3. Import stop words vocabulary and tokenizer</a></li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#2.4\">2.4. Extract the entities in a single file</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#2.5\">2.5. Create entity files</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li><a href=\"#3\">3. Creating the recommendation dataset</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#3.1\">3.1. Import libraries</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#3.2\">3.2. Get all articles id that cannot be considered in use case: blacklist </a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#3.3\">3.3. Create the dataset like < user, item, rating, year> </a></li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#3.4\">3.4. Get entities labels</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#3.5\">3.5. Save data</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li><a href=\"#4\">4. Data statistics</a></li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "## 1. Data retrieval + cleaning\n",
    "\n",
    "**Goal**: To retrieve the [COVID-19 Open Research Dataset (CORD-19)](https://www.semanticscholar.org/cord19) and to select a sample of complete English articles (authors' info, title, body text) to build a scientific recommendation dataset.\n",
    "\n",
    "CORD-19 includes coronavirus-related research articles extracted from several sources, such as PubMed, bioRxiv, medRxiv, WHO."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"1.1\"></a>\n",
    "### 1.1 Import libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sys\n",
    "import langdetect\n",
    "from langdetect import detect\n",
    "sys.path.append(\"./\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"1.2\"></a>\n",
    "### 1.2. Retrieve CORD-19\n",
    "\n",
    "CORD-19 is a large dataset, so in this tutorial we are going to use a smaller version of the dataset. \n",
    "This version is located under the directory \"cord19_small\":"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "os.chdir('data')\n",
    "os.system('tar -xvf cord19_small.tar.xz')\n",
    "os.chdir('../')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, if you want to retrieve the entire dataset, you can run the following code:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "os.chdir('data')\n",
    "         \n",
    "version = 'cord-19_2020-05-12.tar.gz'\n",
    "\n",
    "url = 'https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/' + version\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "os.system('tar -xvf cord-19_2020-05-12.tar.gz')\n",
    "os.chdir('../')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's explore the contents of the dataset directory, particularly, the metadata file."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset_dir = 'data/cord19_small/'\n",
    "metadata_filepath = dataset_dir + 'metadata.csv'\n",
    "\n",
    "metadata = pd.read_csv(metadata_filepath, sep = ',', quotechar = '\"',  encoding = 'utf-8', dtype=str) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have a DataFrame with the contents of the metadata file."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To print column names and first row:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metadata.head(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To access individual rows/articles:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metadata.loc[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
<<<<<<< HEAD
    "To access the individual column title':"
   ],
   "metadata": {}
=======
    "To access the individual column 'title':"
   ]
>>>>>>> 68037e679f997da359c6e7daa779f93a4628c535
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metadata['title']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check the summary statistics:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metadata.count()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "By looking at the statistics, we can see that the number of records with data for the column 'cord_uid' is higher than the number of records with data for the column 'authors'. \n",
    "\n",
    "For our dataset, we only want to include articles with the following characteristics:\n",
    "- authors' information\n",
    "- available title\n",
    "- available body text\n",
    "- article text expressed in English\n",
    "- non-duplicate articles"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"1.3\"></a>\n",
    "\n",
    "### 1.3. Exploring the articles of the dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's consider the first article appearing in the metadata file.\n",
    "To check if information about the article's author is available:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metadata.loc[0]['authors']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To check if there is an available title:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metadata.loc[0]['title']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the title is expressed in English, but it would not be efficient to check the language of every article in the dataset, so we will apply the language detection tool [langdetect](https://pypi.org/project/langdetect/):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "title1 = metadata.loc[0]['title']\n",
    "\n",
    "title1_lang = detect(title1)\n",
    "\n",
    "print(\"Title:\", title1, \"\\nLanguage:\", title1_lang)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The tool detects English as the language of the title.\n",
    "\n",
    "Let's check another article:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "title2 = metadata.loc[199]['title']\n",
    "\n",
    "title2_lang = detect(title2)\n",
    "\n",
    "print(\"Title:\", title2, \"\\nLanguage:\", title2_lang)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this case, the tool detects german ('de') as the language of the title."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we want to check if there is an available abstract for the first article:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metadata.loc[0]['abstract']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The metadata file does not contain the article's text besides abstract and title, but we can access the file associated with the article using the provided information in the columns 'pdf_json_files' or 'pmc_json_files':"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metadata.loc[0]['pdf_json_files']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's open the file, which is in [JSON](https://www.json.org/json-en.html) format:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "article1_filepath = dataset_dir + metadata.loc[0]['pdf_json_files']\n",
    "\n",
    "with open(article1_filepath, encoding='utf-8') as article1_file:\n",
    "    article1_data = json.load(article1_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have the file content stored in a dictionary with the following keys:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "article1_data.keys()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The article content is the following:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "article1_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check the body text:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "body_text = article1_data['body_text']\n",
    "body_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have a list of dictionaries, each dictionary is a paragraph belonging to a given section of the article. We want to join the different parts text in a single string: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "article1_text = str()\n",
    "\n",
    "for paragraph in body_text:\n",
    "    article1_text += paragraph['text'] + '\\n'\n",
    "\n",
    "article1_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "All good! We will include this article in our dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"1.4\"></a>\n",
    "\n",
    "### 1.4. Selecting a sample of articles to build our scientific recommendation dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
<<<<<<< HEAD
    "Instead of repating each operation for each file individually, let us adapt our code to automatically select a sample containing 100 preprocessed articles. "
   ],
   "metadata": {}
=======
    "Instead of repeating each operation for each file individually, let us adapt our code to automatically select a sample containing 100 preprocessed articles. "
   ]
>>>>>>> 68037e679f997da359c6e7daa779f93a4628c535
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, create the output directory:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "out_dir = 'data/sample/'\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, initiallize the necessary variables:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "max_articles = 100 #number of articles to include in the sample\n",
    "dataset_dir = 'data/cord19_small/'\n",
    "metadata_filepath = dataset_dir + 'metadata.csv'\n",
    "valid_articles_count = int()\n",
    "out_articles_ids = list()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Open the metadata file:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metadata = pd.read_csv(metadata_filepath, sep = ',', quotechar = '\"',  encoding = 'utf-8', dtype=str) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then iterate over the records in the metadata file and choose only the relevant ones:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "valid_articles_count = int()\n",
    "blacklist = str()\n",
    "blacklist_count = int()\n",
    "\n",
    "for index, record in metadata.iterrows():\n",
    "    invalid_article = True\n",
    "    \n",
    "    if valid_articles_count <= max_articles:\n",
    "        \n",
    "        if record['pubmed_id'] not in out_articles_ids:\n",
    "            \n",
    "            if type(record['sha']) != float:\n",
    "\n",
    "                if record['authors'] != '':    \n",
    "\n",
    "                    if record['title'] != '':\n",
    "                        title = record['title']\n",
    "                        title_lang = detect(title)\n",
    "                        article_filepath = record['pdf_json_files']\n",
    "                        \n",
    "                        if article_filepath != '': # to consider onyl articles from the pdf_json directory\n",
    "\n",
    "                            if title_lang == 'en'  \\\n",
    "                                and type(article_filepath) != float  \\\n",
    "                                and article_filepath.count(\"document\") == 1:\n",
    "\n",
    "                                article_filepath_up = dataset_dir + record['pdf_json_files']\n",
    "\n",
    "                                with open(article_filepath_up, encoding='utf-8') as article_file:\n",
    "                                    article_data = json.load(article_file)\n",
    "\n",
    "                                if 'body_text' in article_data.keys(): # the article is valid\n",
    "                                    valid_articles_count += 1\n",
    "                                    invalid_article = False\n",
    "\n",
    "                                    # open the article file to check if it contains all info\n",
    "                                    with open(article_filepath_up) as article_file:\n",
    "                                        article_data = json.load(article_file)\n",
    "\n",
    "                                    # correct the info of the article with info present in metadata file\n",
    "                                    changed_article = False\n",
    "\n",
    "                                    if article_data['metadata']['title'] == '':\n",
    "                                        article_data['metadata']['title'] = record['title']\n",
    "                                        changed_article = True\n",
    "\n",
    "                                    if article_data['metadata']['authors'] == []:\n",
    "                                        authors = record['authors'].split(';')   \n",
    "                                        \n",
    "                                        add_author = {\n",
    "                                                     'first': '', 'middle': [], 'last': '',\n",
    "                                                     'suffix': '', 'affiliation': {}, 'email': ''\n",
    "                                                     }\n",
    "                                        authors_up = list()            \n",
    "                                        \n",
    "                                        for author in authors:\n",
    "                                            add_author['last'] = author.split(',')[0]\n",
    "                                            \n",
    "                                            begin_names = author.split(',')[1].split(' ')\n",
    "                                            add_author['first'] = begin_names[1]\n",
    "                                            \n",
    "                                            if len(begin_names) == 3:\n",
    "                                                add_author['middle'] = [begin_names[2]]\n",
    "                                            \n",
    "                                            authors_up.append(add_author)\n",
    "                                            \n",
    "                                        article_data['metadata']['authors'] = authors_up\n",
    "                                        changed_article = True\n",
    "                                        \n",
    "                                    # output or copy article file to out_dir\n",
    "                                    if changed_article:\n",
    "                                      \n",
    "                                        with open(out_dir + record['sha'] + '.json', 'w') as out_file:\n",
    "                                            out_file.write(json.dumps(article_data, indent=4, ensure_ascii=False))\n",
    "\n",
    "                                    else:\n",
    "                                        command = 'cp '  \\\n",
    "                                                  + article_filepath_up + ' ' \\\n",
    "                                                  + out_dir  \\\n",
    "                                                  + record['sha'] + '.json'\n",
    "\n",
    "                                        os.system(command)\n",
    "        \n",
    "        if invalid_article: # store article pubmed id in blacklist file\n",
    "            blacklist += record['pubmed_id'] + \"\\n\"\n",
    "            blacklist_count += 1\n",
    "            \n",
    "    if valid_articles_count == max_articles:\n",
    "        total_articles = index + 1\n",
    "        break\n",
    "\n",
    "#Create blacklist file with info about invalid articles\n",
    "with open('data/blacklist/blacklist_articles.txt', 'w') as blacklist_file:\n",
    "    blacklist_file.write(blacklist)\n",
    "    blacklist_file.close()\n",
    "\n",
    "print(\"Invalid articles:\", str(blacklist_count))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you were not able to run the code, you can uncompress the file 'sample.tar.xz' under 'data' directory."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check if the output directory contain the desired number of articles (max_articles):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "article_count = len(os.listdir(out_dir))\n",
    "assert article_count == max_articles, 'Invalid number of article(s): {}! Expected number: {}'.format(article_count, max_articles)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "At the end of this section, we now have a sample including 100 articles that will be the basis of our scientific recommendation dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2. Named Entity Recognition (NER) + Named Entity Linking (NEL)\n",
    "\n",
    "**Goal**: To recognize chemical and disease entities in the retrieved articles and to link them to the respective ontology identifiers.\n",
    "\n",
    "We are going to use the [Disease Ontology](https://disease-ontology.org/) (DO), and the [Chemical Entities of Biological Interest](https://www.ebi.ac.uk/chebi/) (ChEBI) ontology.\n",
    "\n",
    "To perform NER and NEL, we are going to apply Minimal Named-Entity Recognizer [MER](https://pypi.org/project/merpy/) tool."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"2.1\"></a>\n",
    "### 2.1. Import libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import merpy\n",
    "import multiprocessing\n",
    "from collections import Counter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"2.2\"></a>\n",
    "### 2.2. Configure MER"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we are going to download the ontologies files:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**IMPORTANT:**  Only for OSX\n",
    "see: https://github.com/prisma-labs/python-graphql-client/issues/13"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import urllib.request\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "response = urllib.request.urlopen('https://www.python.org')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "os.chdir('data/ontologies')\n",
    "\n",
    "#Donwload DO (2021-06-03 version)\n",
    "os.system('wget https://raw.githubusercontent.com/DiseaseOntology/HumanDiseaseOntology/v2021-06-03/src/ontology/releases/doid.owl')\n",
    "merpy.download_lexicon('https://raw.githubusercontent.com/DiseaseOntology/HumanDiseaseOntology/v2021-06-03/src/ontology/releases/doid.owl',\n",
    "                       'doid', ltype='owl')\n",
    "\n",
    "#Donwload ChEBI (2021-07-01 version)\n",
    "os.system('wget ftp://ftp.ebi.ac.uk/pub/databases/chebi/archive/rel201/ontology/chebi_lite.owl.gz')\n",
    "os.system ('gzip -d chebi_lite.owl.gz')\n",
    "merpy.download_lexicon('ftp://ftp.ebi.ac.uk/pub/databases/chebi/archive/rel201/ontology/chebi_lite.owl',\n",
    "                       'chebi', ltype='owl')\n",
    "\n",
    "os.chdir('../../')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we need to process the downloaded files into lexicons that MER can use:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "merpy.process_lexicon(\"doid\", ltype=\"owl\")\n",
    "\n",
    "merpy.process_lexicon(\"chebi\", ltype=\"owl\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are going to delete obsolete concepts still present in the ontologies file:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "merpy.delete_obsolete(\"chebi\")\n",
    "\n",
    "merpy.delete_obsolete(\"doid\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check the lexicons available for MER:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "merpy.show_lexicons()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"2.3\"></a>\n",
    "### 2.3. Import stop words vocabulary and tokenizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stop words are common words of a given language (for example the words 'the', 'and', 'in'). A typical pre-processing step is to tokenize the text and remove the stopwords. For that, we are going to import NLTK's list of english stopwords and use the NLTK tokenizer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import nltk \n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "all_stopwords = stopwords.words('english')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are going to extend the stop words vocabulary by adding stop words associated with ChEBI and DO:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "kbs_stopwords = list()\n",
    "blacklist_dir = 'data/blacklist/'\n",
    "filenames = ['chebi.txt', 'doid.txt']\n",
    "\n",
    "for filename in filenames:\n",
    "    \n",
    "    with open(blacklist_dir + filename, 'r') as backlist_file:\n",
    "        stopwords = [content.strip('\\n') for content in backlist_file.readlines()]\n",
    "        kbs_stopwords.extend(stopwords)\n",
    "        backlist_file.close()\n",
    "\n",
    "#Extend stop words vocabulary with the retrieved KBs stopwords\n",
    "all_stopwords.extend(kbs_stopwords)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"2.4\"></a>\n",
    "### 2.4. Extract the entities in a single file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's retrieve a file from the articles sample:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset_dir = 'data/sample/'\n",
    "\n",
    "with open(dataset_dir + '87390d2ae28407b3e03e60a6b24a7fd99ed7229a.json') as article1_file:\n",
    "    article_data = json.load(article1_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check the contents of the article:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "article_data.keys()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to recognize the entities present in title, abstract, and body. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, let's retrieve the title, which is a value associated with the key 'metadata':"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "title = article_data['metadata']['title']\n",
    "title"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's tokenize the title:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "title_tokens = word_tokenize(title)\n",
    "title_tokens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And remove the tokens relative to stop words:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "title_tokens_up = [word for word in title_tokens if not word in all_stopwords]\n",
    "title_tokens_up"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And rebuild the title without the stop words:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "title_up = (' ').join(title_tokens_up)\n",
    "title_up"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we apply MER to the preprocessed title in order to recognize disease entities and to link them to DO concepts:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
<<<<<<< HEAD
    "merpy.get_entities(title_up, 'doid')"
   ],
   "outputs": [],
   "metadata": {}
=======
    "title_entities_up = merpy.get_entities(title_up, 'doid')\n",
    "title_entities_up"
   ]
>>>>>>> 68037e679f997da359c6e7daa779f93a4628c535
  },
  {
   "cell_type": "markdown",
   "source": [
<<<<<<< HEAD
    "Let's check if the annotations make sense. For instance, access the link http://purl.obolibrary.org/obo/DOID_2945."
   ],
   "metadata": {}
=======
    "Let's check if the annotation make sense. For instance, access the link http://purl.obolibrary.org/obo/DOID_2945."
   ]
>>>>>>> 68037e679f997da359c6e7daa779f93a4628c535
  },
  {
   "cell_type": "markdown",
   "source": [
    "The entity 'SARS' in the article was linked to the DO concept 'severe acute respiratory syndrome' with the identifier 'DOID:2945', which seems correct!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "source": [
    "Let's apply MER to recognize chemical entities and to link them to ChEBI concepts:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "merpy.get_entities(title_up, 'chebi')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Accessing the link http://purl.obolibrary.org/obo/CHEBI_35341, we can see that the entity 'Steroids' was linked to the ChEBI concept 'steroid', which has the identifier 'CHEBI:35341'."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We add the disease and chemical entities to a single list:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "title_entities = merpy.get_entities(title_up, 'doid') + merpy.get_entities(title_up, 'chebi')\n",
    "title_entities_up = [entity for entity in title_entities if entity != ['']]\n",
    "\n",
    "title_entities_up"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
=======
   "id": "d02d53bb",
   "metadata": {},
>>>>>>> 68037e679f997da359c6e7daa779f93a4628c535
   "source": [
    "Now, we are going to apply MER to recognize entities in abstract:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "abstract = article_data['abstract'][0]['text']\n",
    "\n",
    "#Tokenize and remove stop words\n",
    "abstract_tokens = word_tokenize(abstract)\n",
    "abstract_tokens_up = [word for word in abstract_tokens if not word in all_stopwords]\n",
    "abstract_up = (' ').join(abstract_tokens_up)\n",
    "\n",
    "#Entity recognition and linking \n",
    "abstract_entities =  merpy.get_entities(abstract_up, 'doid') + merpy.get_entities(abstract_up, 'chebi')\n",
    "abstract_entities_up = [entity for entity in abstract_entities if entity != ['']]\n",
    "\n",
    "abstract_entities_up"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's apply MER in the text associated with the body of the article:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "body = str()\n",
    "\n",
    "for section in article_data['body_text']:\n",
    "    body += section['text'] + \"\\n\"\n",
    "    \n",
    "#Tokenize and remove stop words\n",
    "body_tokens = word_tokenize(body)\n",
    "body_tokens_up = [word for word in body_tokens if not word in all_stopwords]\n",
    "body_up = (' ').join(body_tokens_up)\n",
    "\n",
    "#Entity recognition and linking \n",
    "body_entities =  merpy.get_entities(body_up, \"doid\") + merpy.get_entities(body_up, \"chebi\")\n",
    "body_entities_up = [entity for entity in body_entities if entity != ['']]\n",
    "\n",
    "body_entities_up"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "At last, we need to obtain information about the frequency of each ontology identifier in the document:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "total_entities = title_entities_up + abstract_entities_up + body_entities_up\n",
    "\n",
    "all_uris = [entity[3] for entity in total_entities]\n",
    "\n",
    "entity_counter = Counter(all_uris)\n",
    "\n",
    "entity_counter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To sort the URIs by descending order:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "entity_counter = {\n",
    "    k: v \n",
    "    for k, v in sorted(entity_counter.items(), key=lambda item: item[1], reverse=True)\n",
    "    }\n",
    "\n",
    "entity_counter "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"2.5\"></a>\n",
    "### 2.5. Create entity files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to adapt our code to perform NER and NEL in all documents of our sample."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, create the output dir:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "out_dir = 'data/sample_entities/'\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we are going to iterate over each file present in the sample directory, annotate it, and create the respective entity file:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def annotate_doc(article):\n",
    "    \n",
    "    article_filepath = 'data/sample/' + article  \n",
    "\n",
    "    with open(article_filepath) as input_file:\n",
    "        article_data = json.load(input_file)\n",
    "\n",
    "    doc_output = {'id': str(), 'entities': {}, 'sections': {'title': [], 'abstract': [], 'body': []}}\n",
    "    \n",
    "    doc_output['id'] = article_data['paper_id']\n",
    "   \n",
    "    #Annotate the title\n",
    "    title = article_data['metadata']['title']\n",
    "    \n",
    "    #Tokenize and remove stop words\n",
    "    title_tokens = word_tokenize(title)\n",
    "    title_tokens_up = [word for word in title_tokens if not word in all_stopwords]\n",
    "    title_up = (' ').join(title_tokens_up)\n",
    "    \n",
    "    #Entity recognition and linking\n",
    "    title_entities = merpy.get_entities(title_up, 'doid') + merpy.get_entities(title_up, 'chebi')\n",
    "    doc_output['sections']['title'] = [entity for entity in title_entities if entity != ['']]\n",
    "    \n",
    "    #Annotate the abstract\n",
    "    if article_data['abstract'] != []:\n",
    "        abstract = article_data['abstract'][0]['text']\n",
    "        \n",
    "        #Tokenize and remove stop words\n",
    "        abstract_tokens = word_tokenize(abstract)\n",
    "        abstract_tokens_up = [word for word in abstract_tokens if not word in all_stopwords]\n",
    "        abstract_up = (' ').join(abstract_tokens_up)\n",
    "        \n",
    "        #Entity recognition and linking\n",
    "        abstract_entities = merpy.get_entities(abstract_up, 'doid') + merpy.get_entities(abstract_up, 'chebi')\n",
    "        doc_output['sections']['abstract'] = [entity for entity in abstract_entities if entity != ['']]\n",
    "    \n",
    "    else:\n",
    "        abstract_entities = []\n",
    "\n",
    "    #Combine the several paragraphs of the body text and annotate it\n",
    "    body = str()\n",
    "\n",
    "    for section in article_data['body_text']:\n",
    "        body += section['text'] + '\\n'\n",
    "    \n",
    "    #Tokenize and remove stop words\n",
    "    body_tokens = word_tokenize(body)\n",
    "    body_tokens_up = [word for word in body_tokens if not word in all_stopwords]\n",
    "    body_up = (' ').join(body_tokens_up)    \n",
    "        \n",
    "    #Entity recognition and linking\n",
    "    body_entities = merpy.get_entities(body_up, 'doid') + merpy.get_entities(body_up, 'chebi')\n",
    "    doc_output['sections']['body'] = [entity for entity in body_entities if entity != ['']]\n",
    "\n",
    "    # Count URIs frequencies and sort them\n",
    "    total_entities = title_entities + abstract_entities + body_entities\n",
    "    all_uris = [entity[3] for entity in total_entities if len(entity)==4]\n",
    "    entity_counter = Counter(all_uris)\n",
    "    \n",
    "    doc_output['entities'] = {\n",
    "        k: v \n",
    "        for k, v in sorted(entity_counter.items(), key=lambda item: item[1], reverse=True)\n",
    "        }\n",
    "    \n",
    "    #Generate JSON file with output\n",
    "    out_filepath = out_dir + doc_output['id'] + '_entities.json'\n",
    "    \n",
    "    with open(out_filepath, 'w') as out_file:\n",
    "        out_file.write(json.dumps(doc_output, indent=4))\n",
    "        \n",
    "\n",
    "article_dir = 'data/sample/'\n",
    "        \n",
    "with multiprocessing.Pool(processes=10) as pool: \n",
    "    # change the number of processes according to number of available cores\n",
    "    outputs = pool.map(annotate_doc, [article for article in os.listdir(article_dir)], chunksize=10)\n",
    "    pool.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**IMPORTANT:** If you were not able to run the previous code, you can extract the file 'sample_entities.tar.xz' under 'data' directory"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "os.chdir('data')\n",
    "os.system('tar -xvf sample_entities.tar.xz')\n",
    "os.chdir('../')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have both the article files ('data/sample' directory) and the respective entities files ('data/sample_entities' directory), and the next step will be the generation of the scientific recomendation dataset using the LIBRETTI algorithm."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. Create the recommendation dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"3.1\"></a>\n",
    "### 3.1. Import libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import unidecode\n",
    "\n",
    "from pathlib import Path\n",
    "import rdflib\n",
    "from rdflib import URIRef"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"max_rows\", None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Path of original' json and entities' json folder, blacklist and metadata"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset_dir = 'data/cord19_small/document_parses/pdf_json/'\n",
    "entities_dir = 'data/sample_entities/'\n",
    "metadata_filepath = 'data/cord19_small/metadata.csv'\n",
    "blacklist_filepath = 'data/blacklist/blacklist_articles.txt'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "List containing the names of the entries in the directory given by path"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "entities_list_of_json_files = os.listdir(entities_dir)\n",
    "print(entities_list_of_json_files)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"3.2\"></a>\n",
    "### 3.2. Get all articles id that cannot be considered in use case: blacklist "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Return all articles to be removed, due some errors found there\n",
    "\n",
    "articles_blacklist = []\n",
    "with open(blacklist_filepath, 'r') as f:\n",
    "    black_list = [content for content in f.readlines()]\n",
    "\n",
    "articles_blacklist.extend(black_list) \n",
    "f.close()\n",
    "\n",
    "articles_blacklist"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Open metadata file:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metadata = pd.read_csv(metadata_filepath, sep = ',', quotechar = '\"',  encoding = 'utf-8', dtype=str) \n",
    "metadata.iloc[:].head(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"3.3\"></a>\n",
    "### 3.3. Create the dataset like <user, item, rating, year> "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "user_item_rating_all = []\n",
    "count = 0\n",
    "\n",
    "#e.g.\n",
    "#entities_list_of_json_files=['23bc55d6f63fab18b02004483888db2b6a0bfa48_entities.json']\n",
    "\n",
    "for file in entities_list_of_json_files:\n",
    "    \n",
    "    if file.replace('_entities.json','') in articles_blacklist:\n",
    "        continue\n",
    "     \n",
    "    print(count, \"-\", len(entities_list_of_json_files))            \n",
    "    print(file)\n",
    "        \n",
    "    # check valid json file, i.e. contains values\n",
    "    try:\n",
    "        j_file_entities = pd.read_json(entities_dir + file, orient = 'index')    \n",
    "    except Exception as e:\n",
    "        print(f'Json file does not contain values. Error message {e}')\n",
    "        #This function receives the article to and saves them in the\n",
    "        # blacklist file. The backlist contains all invalid articles, such\n",
    "        # as non-authors, non-entities, and others\n",
    "        articles_blacklist.extend(file.replace('_entities.json','').rstrip('\\r\\n') + '\\n')\n",
    "        continue  \n",
    "\n",
    "    my_dict = j_file_entities.loc['entities'][0]  \n",
    "    df_entities = pd.DataFrame(my_dict.items(), columns=['entities', 'count'])\n",
    "    \n",
    "    df_entities['entities_id'] = df_entities.entities.str.split(pat=\"/\").str[-1]\n",
    "   \n",
    "    if df_entities.empty:\n",
    "        print(f'Json file does not contain values.')\n",
    "        articles_blacklist.extend(file.replace('_entities.json','').rstrip('\\r\\n') + '\\n')\n",
    "        continue     \n",
    "    \n",
    "    #print(df_entities)\n",
    "    \n",
    "    article_id = j_file_entities.loc['id'].values[0]\n",
    "    \n",
    "    # check valid json file, i.e. contains values\n",
    "    try:\n",
    "        # Convert a JSON string to pandas object, and return a dataframe\n",
    "        with open(dataset_dir + article_id + '.json', encoding='utf-8') as json_file:\n",
    "            j_file_original = json.load(json_file)\n",
    "        #print(j_file_original)\n",
    "    except Exception as e:\n",
    "        print(f'Original json file does not exist. Error message {e}')\n",
    "        articles_blacklist.extend(file.replace('_entities.json','').rstrip('\\r\\n') + '\\n')\n",
    "        continue    \n",
    "    \n",
    "    # check if json file contains AUTHORS, otherwise try to find them in metadata.csv\n",
    "    # if value remains null them put this article in the blacklist file\n",
    "    \n",
    "    list_of_authors = []\n",
    "    for p in j_file_original['metadata']['authors']:\n",
    "        \n",
    "        if len(p['first']) == 0 or len(p['last']) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            # remove all characters except alphabets from a string to unidecode\n",
    "            first = unidecode.unidecode( ''.join(m for m in p['first'] if m.isalpha()))\n",
    "            middle = unidecode.unidecode( ''.join(m for m in p['middle'] if m.isalpha()))\n",
    "            last = unidecode.unidecode( ''.join(m for m in p['last'] if m.isalpha()))\n",
    "\n",
    "            list_of_authors.append(last + ', '+ first + ' '+ middle)\n",
    "    \n",
    "###  ---- Begin of Metadata file ----\n",
    "    \n",
    "    # CHECK AUTHORS\n",
    "    \n",
    "    # if authors is empty we will find in metadata.csv file\n",
    "    if len(list_of_authors)==0:\n",
    "        ##if string is NaN\n",
    "        try:\n",
    "            authors = data[metadata.sha == article_id].authors.values[0].split(';')\n",
    "            for a in authors:\n",
    "                a = a.split(',')\n",
    "                first = unidecode.unidecode( ''.join(m for m in a[1] if m.isalpha()))\n",
    "                last = unidecode.unidecode( ''.join(m for m in a[0] if m.isalpha()))\n",
    "                list_of_authors.append(last + ', '+  first) \n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f'Empty values {e}')   \n",
    "            articles_blacklist.extend(file.replace('_entities.json','').rstrip('\\r\\n') + '\\n')             \n",
    "        continue\n",
    "        \n",
    "    # CHECK YEAR\n",
    "    \n",
    "    publish_date=''      \n",
    "    try:\n",
    "        publish_date = metadata[metadata.sha == article_id].\\\n",
    "            publish_time.map(lambda v: v.split('-')[0]).tolist()[0]\n",
    "    finally:\n",
    "        pass  \n",
    "    #print(publish_date)\n",
    "    \n",
    "    if publish_date==None:\n",
    "        articles_blacklist.extend(file.replace('_entities.json','').rstrip('\\r\\n') + '\\n')\n",
    "        continue\n",
    "\n",
    "    # put more articles ids in the blacklist file    \n",
    "    with open(blacklist_filepath, 'r+') as blacklist_file:\n",
    "            content = blacklist_file.read()\n",
    "            blacklist_file.seek(0, 0)\n",
    "            blacklist_file.write(' '.join(map(str, articles_blacklist)) )\n",
    "            blacklist_file.write(content)\n",
    "            blacklist_file.close()  \n",
    "            \n",
    "    count+=1\n",
    "    \n",
    "###  ---- End of Metadata file ----            \n",
    "    \n",
    "    # Append <user, item, rating> tuple\n",
    "    user_item_rating = []\n",
    "    for author in list_of_authors:\n",
    "        for entity in df_entities.entities_id:\n",
    "            user_item_rating.append([author, entity, 1])\n",
    "\n",
    "    #print(user_item_rating)\n",
    "    \n",
    "    ## add publish_date in array in index column = 3\n",
    "    user_item_rating = np.insert(user_item_rating, 3, publish_date, axis=1)\n",
    "    #print(user_item_rating)\n",
    "    \n",
    "    user_item_rating_all.append(user_item_rating)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "flat_list = []\n",
    "for sublist in user_item_rating_all:\n",
    "    for item in sublist:\n",
    "        flat_list.append(item)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Anonymization of the author's name: adding an userid to the author's name of the article <author_name, item, rating, year, user>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_data = pd.DataFrame(np.array(flat_list),  columns=['user', 'item', 'rating', 'year'])\n",
    "sum_df = final_data.groupby(['user', 'item', 'year']).size().reset_index().rename(columns={0: 'rating'})   \n",
    "\n",
    "#     maps the values to the lowest consecutive values\n",
    "df_user_index = pd.DataFrame(sum_df.user.unique(), columns=[\"user\"])\n",
    "df_user_index[\"new_index\"] = np.arange(0, len(sum_df.user.unique()))\n",
    "\n",
    "sum_df[\"index_user\"] = sum_df[\"user\"].map(df_user_index.set_index('user')[\"new_index\"]).fillna(0) \n",
    "df_with_user_id = sum_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_with_user_id.rename(columns={'user': 'author_name', 'index_user': 'user'}, inplace = True)\n",
    "df_with_user_id"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"3.4\"></a>\n",
    "### 3.4. Get entities labels"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "list_of_entities = df_with_user_id.item.unique()\n",
    "print(list_of_entities) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we create a graph, a representation of the ontology with **RDFLib library**\n",
    "\n",
    "**Concept of RDF**\n",
    "The RDF is a standard graph-based representation format that stores semantic facts. I.e., RDF is a model for data publishing and interchange on the Web standardized by W3C. It is also used in semantic graph databases (also known as RDF triplestores). \n",
    "\n",
    "RDF triplestore databases are successfully used for managing **Linked Open Data** datasets.\n",
    "\n",
    "**Linked Open Data** is a set of design principles for sharing machine-readable interlinked Open Data on the Web.\n",
    "\n",
    "<center><img src=\"img/Linked-Data.png\" width=\"300\" ><br> (Source: https://www.ontotext.com)</center>\n",
    "\n",
    "As the HTTP protocol provides a simple mechanism for retrieving resources, when things can be identified by URIs in conjunction with this protocol, they become easier to find. This expedites publishing any kind of data and adding it to the global data space.\n",
    "\n",
    "The Uniform Resource Identifier (URI) is a unique sequence of characters used for giving unique names to anything – from digital content available on the Web to real-world objects and abstract concepts. With the help of URIs, we can distinguish between different things or know that one thing from one dataset is the same as another in a different dataset.\n",
    "\n",
    "Linked Data is one of the core pillars of the **Semantic Web**:\n",
    "\n",
    "<center><img src=\"img/semanticweb.png\" width=\"250\" ><br> (Source: wikipedia)</center>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "os.chdir('data/ontologies')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "chebi = rdflib.Graph()\n",
    "print('Loading ... chebi')\n",
    "chebi.load('chebi_lite.owl')\n",
    "\n",
    "doid = rdflib.Graph()\n",
    "print('Loading ... doid')   \n",
    "doid.load('doid.owl')\n",
    "\n",
    "print('Successful loading!')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "os.chdir('../../')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The entities labels are available in http://purl.obolibrary.org/obo/ based on items prefix\n",
    "\n",
    "e.g., considering 'CHEBI:35341', the corresponding link is http://purl.obolibrary.org/obo/CHEBI_35341, and the label found is the entity 'steroid'. I.e., the identifier 'CHEBI:35341' was linked to the ChEBI concept 'steroid'.\n",
    "\n",
    "<center><img src=\"img/steroid_chebi.png\" width=400><img src=\"img/steroid_graphview.png\" width=\"400\" ><br> </center>\n",
    "\n",
    "Now define the key words that we will use (the edge weights of the graph)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "entities_label = []\n",
    "for id in list_of_entities: \n",
    "    uri = URIRef('http://purl.obolibrary.org/obo/' + id)\n",
    "    if id.startswith('CHEBI'):\n",
    "         lab = chebi.label(uri)\n",
    "    elif id.startswith('DO'):\n",
    "        lab = doid.label(uri)\n",
    "    entities_label.append(lab)\n",
    "\n",
    "entities_label"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_entities = pd.DataFrame(list_of_entities, columns=[\"item_id\"])\n",
    "df_entities[\"entity_name\"] = np.array(entities_label)\n",
    "df_entities"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mapping labels, and get dataset as <author_name, item, year, rating, user, item_name>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_with_user_id[\"item_name\"] = df_with_user_id[\"item\"].map(df_entities.set_index('item_id')[\"entity_name\"]).fillna(0)\n",
    "df_with_user_id"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"3.5\"></a>\n",
    "### 3.5. Save data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, create the output dir "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cord_ds_dir = 'data/results/sample_cord-19_ds.csv'\n",
    "cord_userid_dir = 'data/results/sample_cord-19_ds_userid.csv'\n",
    "\n",
    "if not os.path.exists(os.path.dirname(cord_ds_dir)):\n",
    "        Path(os.path.dirname(cord_ds_dir)).mkdir(parents=True, exist_ok=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, save all values in csv file        "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_with_user_id[['user', 'item', 'rating', 'item_name', 'year']].to_csv(cord_ds_dir, index=False, header=False)\n",
    "\n",
    "df_with_user_id[['user', 'author_name']].to_csv(cord_userid_dir, index=False, header=False) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sample_cord = pd.read_csv(cord_ds_dir, sep = ',', quotechar = '\"',  encoding = 'utf-8', dtype=str, header=None) \n",
    "sample_cord.iloc[:].head(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4. Data statistics "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('data/results/sample_cord-19_ds.csv', names=['user', 'item', 'rating', 'item_name', 'year'])\n",
    "print(dataset.head(20))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# number of unique users\n",
    "# number of unique items\n",
    "# number of ratings\n",
    "# sparsity \n",
    "\n",
    "print('n users: ', dataset.user.unique().shape[0])\n",
    "print('n items: ', dataset.item.unique().shape[0])\n",
    "print('n ratings: ', dataset.size)\n",
    "\n",
    "print('sparsity: ', 1 - (dataset.size / (dataset.user.unique().shape[0] * dataset.item.unique().shape[0])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# items by ontology\n",
    "\n",
    "print(\"CHEBI items: \", dataset[dataset.item.str.startswith('CHEBI')].item.unique().shape[0])\n",
    "\n",
    "print(\"DOID items: \", dataset[dataset.item.str.startswith('DOID')].item.unique().shape[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# items by user\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "unique_users = dataset.user.unique()\n",
    "items_by_user = dataset.groupby(['user'])[\"item\"].count().reset_index()\n",
    "items_by_user = items_by_user.sort_values(by=['item'], ascending=False)\n",
    "print(items_by_user)\n",
    "items_by_user.user = items_by_user.user.astype('str')\n",
    "\n",
    "print('max items by user: ', items_by_user.item.max())\n",
    "print('min items by user: ', items_by_user.item.min())\n",
    "print('mean items by user: ', items_by_user.item.mean())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.scatter(items_by_user.user, items_by_user.item)\n",
    "plt.axhline(y=items_by_user.item.mean(), color='r', linestyle='-')\n",
    "plt.ylabel('number of items')\n",
    "plt.xlabel('user')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# users by item\n",
    "\n",
    "unique_items = dataset.item.unique()\n",
    "users_by_item = dataset.groupby(['item'])[\"user\"].count().reset_index()\n",
    "users_by_item = users_by_item.sort_values(by=['user'], ascending=False)\n",
    "\n",
    "print('max users by item: ', users_by_item.user.max())\n",
    "list_of_max_items = users_by_item[users_by_item.user == users_by_item.user.max()].item.values\n",
    "print(list_of_max_items)\n",
    "\n",
    "print(dataset[dataset.item == list_of_max_items[0]])\n",
    "\n",
    "print('min users by item: ', users_by_item.user.min())\n",
    "print('mean users by item: ', users_by_item.user.mean())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset_doid = dataset[dataset.item.str.startswith('DOID')]\n",
    "unique_items_d = dataset_doid.item.unique()\n",
    "users_by_item_d = dataset_doid.groupby(['item'])[\"user\"].count().reset_index()\n",
    "users_by_item_d = users_by_item_d.sort_values(by=['user'], ascending=False)\n",
    "\n",
    "print('max users by item: ', users_by_item_d.user.max())\n",
    "list_of_max_items_d = users_by_item_d[users_by_item_d.user == users_by_item_d.user.max()].item.values\n",
    "print(list_of_max_items_d)\n",
    "\n",
    "print(dataset_doid[dataset_doid.item == list_of_max_items_d[0]])\n",
    "\n",
    "print('min users by item: ', users_by_item_d.user.min())\n",
    "print('mean users by item: ', users_by_item_d.user.mean())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset_chebi = dataset[dataset.item.str.startswith('CHEBI')]\n",
    "unique_items_c = dataset_chebi.item.unique()\n",
    "users_by_item_c = dataset_chebi.groupby(['item'])[\"user\"].count().reset_index()\n",
    "users_by_item_c = users_by_item_c.sort_values(by=['user'], ascending=False)\n",
    "\n",
    "print('max users by item: ', users_by_item_c.user.max())\n",
    "list_of_max_items_c = users_by_item_c[users_by_item_c.user == users_by_item_c.user.max()].item.values\n",
    "print(list_of_max_items_c)\n",
    "\n",
    "print(dataset_chebi[dataset_chebi.item == list_of_max_items_c[0]])\n",
    "\n",
    "print('min users by item: ', users_by_item_c.user.min())\n",
    "print('mean users by item: ', users_by_item_c.user.mean())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#%matplotlib qt\n",
    "\n",
    "plt.scatter(users_by_item.item, users_by_item.user)\n",
    "plt.axhline(y=users_by_item.user.mean(), color='r', linestyle='-')\n",
    "#plt.yscale('log')\n",
    "plt.ylabel('number of users')\n",
    "plt.xlabel('items')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys_scifi",
   "language": "python",
   "name": "recsys_scifi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
