{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "removed-florida",
   "metadata": {},
   "source": [
    "# Tutorial: Creating Recommender Systems Datasets in Scientific Fields\n",
    "\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#1\">1. Data retrieval and cleaning</a></li>\n",
    "</ul>\n",
    "   \n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#1.1\">1.1.Import libraries</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#1.2\">1.2. Retrieve CORD-19</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#1.3\">1.3. Exploring the articles of the dataset</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#1.4\">1.4. Selecting a sample of articles to build our scientific recommendation dataset</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li><a href=\"#2\">2. Named Entity Recognition (NER) + Named Entity Linking (NEL)</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#2.1\">2.1. Import libraries</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#2.2\">2.2. Configure MER</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#2.3\">2.3. Import stop words vocabulary and tokenizer</a></li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#2.4\">2.4. Extract the entities in a single file</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#2.5\">2.5. Create entity files</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li><a href=\"#3\">3. Creating the recommendation dataset</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#3.1\">3.1. Import libraries</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#3.2\">3.2. Import user-defined-functions</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#3.3\">3.3. Get entities labels</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#4.4\">3.4. Loading ontologies</a></li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "   <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#3.5\">3.5. Save data</a></li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-disaster",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "## 1. Data retrieval + cleaning\n",
    "\n",
    "**Goal**: To retrieve the [COVID-19 Open Research Dataset (CORD-19)](https://www.semanticscholar.org/cord19) and to select a sample of complete English articles (authors' info, title, body text) to build a scientific recommendation dataset.\n",
    "\n",
    "CORD-19 includes coronavirus-related research articles extracted from several sources, such as PubMed, bioRxiv, medRxiv, WHO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-fountain",
   "metadata": {},
   "source": [
    "<a id=\"1.1\"></a>\n",
    "### 1.1 Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sys\n",
    "import langdetect\n",
    "from langdetect import detect\n",
    "sys.path.append(\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-program",
   "metadata": {},
   "source": [
    "<a id=\"1.2\"></a>\n",
    "### 1.2. Retrieve CORD-19\n",
    "\n",
    "CORD-19 is a large dataset, so in this tutorial we are going to use a smaller version of the dataset. \n",
    "This version is located under the directory \"cord19_small\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('data')\n",
    "os.system('tar -xvf cord19_small.tar.xz')\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-state",
   "metadata": {},
   "source": [
    "However, if you want to retrieve the entire dataset, you can run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-lounge",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('data')\n",
    "         \n",
    "version = 'cord-19_2020-05-12.tar.gz'\n",
    "\n",
    "url = 'https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/' + version\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "os.system('tar -xvf cord-19_2020-05-12.tar.gz')\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-lighter",
   "metadata": {},
   "source": [
    "Let's explore the contents of the dataset directory, particularly, the metadata file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-cooperative",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'data/cord19_small/'\n",
    "metadata_filepath = dataset_dir + 'metadata.csv'\n",
    "\n",
    "metadata = pd.read_csv(metadata_filepath, sep = ',', quotechar = '\"',  encoding = 'utf-8', dtype=str) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-visibility",
   "metadata": {},
   "source": [
    "Now we have a DataFrame with the contents of the metadata file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-penalty",
   "metadata": {},
   "source": [
    "To print column names and first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-finance",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-tribute",
   "metadata": {},
   "source": [
    "To access individual rows/articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-apparatus",
   "metadata": {},
   "source": [
    "To access the individual column title':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-terrain",
   "metadata": {},
   "source": [
    "Let's check the summary statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-implement",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-florist",
   "metadata": {},
   "source": [
    "By looking at the statistics, we can see that the number of records with data for the column 'cord_uid' is higher than the number of records with data for the column 'authors'. \n",
    "\n",
    "For our dataset, we only want to include articles with the following characteristics:\n",
    "- authors' information\n",
    "- available title\n",
    "- available body text\n",
    "- article text expressed in English\n",
    "- non-duplicate articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-twins",
   "metadata": {},
   "source": [
    "<a id=\"1.3\"></a>\n",
    "\n",
    "### 1.3. Exploring the articles of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-consultation",
   "metadata": {},
   "source": [
    "Let's consider the first article appearing in the metadata file.\n",
    "To check if information about the article's author is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.loc[0]['authors']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-wonder",
   "metadata": {},
   "source": [
    "To check if there is an available title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.loc[0]['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-belly",
   "metadata": {},
   "source": [
    "We can see that the title is expressed in English, but it would not be efficient to check the language of every article in the dataset, so we will apply the language detection tool [langdetect](https://pypi.org/project/langdetect/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-aruba",
   "metadata": {},
   "outputs": [],
   "source": [
    "title1 = metadata.loc[0]['title']\n",
    "\n",
    "title1_lang = (title1)\n",
    "\n",
    "print(\"Title:\", title1, \"\\nLanguage:\", title1_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-bargain",
   "metadata": {},
   "source": [
    "The tool detects English as the language of the title.\n",
    "\n",
    "Let's check another article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-saturday",
   "metadata": {},
   "outputs": [],
   "source": [
    "title2 = metadata.loc[199]['title']\n",
    "\n",
    "title2_lang = detect(title2)\n",
    "\n",
    "print(\"Title:\", title2, \"\\nLanguage:\", title2_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-lender",
   "metadata": {},
   "source": [
    "In this case, the tool detects german ('de') as the language of the title."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-intensity",
   "metadata": {},
   "source": [
    "Now, we want to check if there is an available abstract for the first article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-treasurer",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.loc[0]['abstract']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-cornell",
   "metadata": {},
   "source": [
    "The metadata file does not contain the article's text besides abstract and title, but we can access the file associated with the article using the provided information in the columns 'pdf_json_files' or 'pmc_json_files':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.loc[0]['pdf_json_files']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-natural",
   "metadata": {},
   "source": [
    "Let's open the file, which is in [JSON](https://www.json.org/json-en.html) format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "article1_filepath = dataset_dir + metadata.loc[0]['pdf_json_files']\n",
    "\n",
    "with open(article1_filepath, encoding='utf-8') as article1_file:\n",
    "    article1_data = json.load(article1_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-thanksgiving",
   "metadata": {},
   "source": [
    "Now we have the file content stored in a dictionary with the following keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-hawaii",
   "metadata": {},
   "outputs": [],
   "source": [
    "article1_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-detail",
   "metadata": {},
   "source": [
    "The article content is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "article1_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-illustration",
   "metadata": {},
   "source": [
    "Let's check the body text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_text = article1_data['body_text']\n",
    "body_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-meeting",
   "metadata": {},
   "source": [
    "We have a list of dictionaries, each dictionary is a paragraph belonging to a given section of the article. We want to join the different parts text in a single string: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "article1_text = str()\n",
    "\n",
    "for paragraph in body_text:\n",
    "    article1_text += paragraph['text'] + '\\n'\n",
    "\n",
    "article1_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-chapter",
   "metadata": {},
   "source": [
    "All good! We will include this article in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-fancy",
   "metadata": {},
   "source": [
    "<a id=\"1.4\"></a>\n",
    "\n",
    "### 1.4. Selecting a sample of articles to build our scientific recommendation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-senate",
   "metadata": {},
   "source": [
    "Instead of repating each operation for each file individually, let us adapt our code to automatically select a sample containing 100 preprocessed articles. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-discharge",
   "metadata": {},
   "source": [
    "First, create the output directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-width",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'data/sample/'\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-leather",
   "metadata": {},
   "source": [
    "Then, initiallize the necessary variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-sitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_articles = 100 #number of articles to include in the sample\n",
    "dataset_dir = 'data/cord19_small/'\n",
    "metadata_filepath = dataset_dir + 'metadata.csv'\n",
    "valid_articles_count = int()\n",
    "out_articles_ids = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-infrared",
   "metadata": {},
   "source": [
    "Open the metadata file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(metadata_filepath, sep = ',', quotechar = '\"',  encoding = 'utf-8', dtype=str) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-little",
   "metadata": {},
   "source": [
    "Then iterate over the records in the metadata file and choose only the relevant ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-dinner",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_articles_count = int()\n",
    "blacklist = str()\n",
    "blacklist_count = int()\n",
    "\n",
    "for index, record in metadata.iterrows():\n",
    "    invalid_article = True\n",
    "    \n",
    "    if valid_articles_count <= max_articles:\n",
    "        \n",
    "        if record['pubmed_id'] not in out_articles_ids:\n",
    "            \n",
    "            if type(record['sha']) != float:\n",
    "\n",
    "                if record['authors'] != '':    \n",
    "\n",
    "                    if record['title'] != '':\n",
    "                        title = record['title']\n",
    "                        title_lang = detect(title)\n",
    "                        article_filepath = record['pdf_json_files']\n",
    "                        \n",
    "                        if article_filepath != '': # to consider onyl articles from the pdf_json directory\n",
    "\n",
    "                            if title_lang == 'en'  \\\n",
    "                                and type(article_filepath) != float  \\\n",
    "                                and article_filepath.count(\"document\") == 1:\n",
    "\n",
    "                                article_filepath_up = dataset_dir + record['pdf_json_files']\n",
    "\n",
    "                                with open(article_filepath_up, encoding='utf-8') as article_file:\n",
    "                                    article_data = json.load(article_file)\n",
    "\n",
    "                                if 'body_text' in article_data.keys(): # the article is valid\n",
    "                                    valid_articles_count += 1\n",
    "                                    invalid_article = False\n",
    "\n",
    "                                    # open the article file to check if it contains all info\n",
    "                                    with open(article_filepath_up) as article_file:\n",
    "                                        article_data = json.load(article_file)\n",
    "\n",
    "                                    # correct the info of the article with info present in metadata file\n",
    "                                    changed_article = False\n",
    "\n",
    "                                    if article_data['metadata']['title'] == '':\n",
    "                                        article_data['metadata']['title'] = record['title']\n",
    "                                        changed_article = True\n",
    "\n",
    "                                    if article_data['metadata']['authors'] == []:\n",
    "                                        article_data['metadata']['authors'] = record['authors']\n",
    "\n",
    "                                    # output or copy article file to out_dir\n",
    "                                    if changed_article:\n",
    "\n",
    "                                        with open(out_dir + record['sha'] + '.json', 'w') as out_file:\n",
    "                                            out_file.write(json.dumps(article_data, indent=4, ensure_ascii=False))\n",
    "\n",
    "                                    else:\n",
    "                                        command = 'cp '  \\\n",
    "                                                  + article_filepath_up + ' ' \\\n",
    "                                                  + out_dir  \\\n",
    "                                                  + record['sha'] + '.json'\n",
    "\n",
    "                                        os.system(command)\n",
    "        \n",
    "        if invalid_article: # store article pubmed id in blacklist file\n",
    "            blacklist += record['pubmed_id'] + \"\\n\"\n",
    "            blacklist_count += 1\n",
    "            \n",
    "    if valid_articles_count == max_articles:\n",
    "        total_articles = index + 1\n",
    "        break\n",
    "\n",
    "#Create blacklist file with info about invalid articles\n",
    "with open('data/blacklist/blacklist_articles.txt', 'w') as blacklist_file:\n",
    "    blacklist_file.write(blacklist)\n",
    "    blacklist_file.close()\n",
    "\n",
    "print(\"Invalid articles:\", str(blacklist_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-music",
   "metadata": {},
   "source": [
    "If you were not able to run the code, you can uncompress the file 'sample.tar.xz' under 'data' directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-victoria",
   "metadata": {},
   "source": [
    "Let's check if the output directory contain the desired number of articles (max_articles):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-overall",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_count = len(os.listdir(out_dir))\n",
    "assert article_count == max_articles, 'Invalid number of article(s): {}! Expected number: {}'.format(article_count, max_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-copper",
   "metadata": {},
   "source": [
    "At the end of this section, we now have a sample including 100 articles that will be the basis of our scientific recommendation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-taylor",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2. Named Entity Recognition (NER) + Named Entity Linking (NEL)\n",
    "\n",
    "**Goal**: To recognize chemical and disease entities in the retrieved articles and to link them to the respective ontology identifiers.\n",
    "\n",
    "We are going to use the [Disease Ontology](https://disease-ontology.org/) (DO), and the [Chemical Entities of Biological Interest](https://www.ebi.ac.uk/chebi/) (ChEBI) ontology.\n",
    "\n",
    "To perform NER and NEL, we are going to apply Minimal Named-Entity Recognizer [MER](https://pypi.org/project/merpy/) tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-kelly",
   "metadata": {},
   "source": [
    "<a id=\"2.1\"></a>\n",
    "### 2.1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "import merpy\n",
    "import multiprocessing\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-semiconductor",
   "metadata": {},
   "source": [
    "<a id=\"2.2\"></a>\n",
    "### 2.2. Configure MER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-gabriel",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install merpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-application",
   "metadata": {},
   "source": [
    "First, we need to download the owl file associated with ChEBI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "merpy.download_lexicon(\"ftp://ftp.ebi.ac.uk/pub/databases/chebi/ontology/chebi.owl\",\n",
    "                       \"chebi\", ltype=\"owl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-marine",
   "metadata": {},
   "source": [
    "Then, we need to process the downloaded file into a lexicon that MER can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-occasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "merpy.process_lexicon(\"chebi\", ltype=\"owl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-opportunity",
   "metadata": {},
   "source": [
    "We are going to delete obsolete concepts still present in the ontology file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-cycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "merpy.delete_obsolete(\"chebi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-nylon",
   "metadata": {},
   "source": [
    "We need to repeat the operations for the DO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-barbados",
   "metadata": {},
   "outputs": [],
   "source": [
    "merpy.download_lexicon(\"http://purl.obolibrary.org/obo/doid.owl\", \n",
    "                        \"do\", ltype=\"owl\")\n",
    "            \n",
    "merpy.process_lexicon(\"do\", ltype=\"owl\")\n",
    "\n",
    "merpy.delete_obsolete(\"do\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-cuisine",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install certifi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-official",
   "metadata": {},
   "source": [
    "Let's check the lexicons available for MER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "merpy.show_lexicons()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-saver",
   "metadata": {},
   "source": [
    "<a id=\"2.3\"></a>\n",
    "### 2.3. Import stop words vocabulary and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-fortune",
   "metadata": {},
   "source": [
    "Stop words are common words of a given language (for example the words 'the', 'and', 'in'). A typical pre-processing step is to tokenize the text and remove the stopwords. For that, we are going to import NLTK's list of english stopwords and use the NLTK tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-shooting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "all_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-retention",
   "metadata": {},
   "source": [
    "We are going to extend the stop words vocabulary by adding stop words associated with ChEBI and DO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-coast",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-underwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "kbs_stopwords = list()\n",
    "blacklist_dir = 'data/blacklist/'\n",
    "filenames = ['chebi.txt', 'doid.txt']\n",
    "\n",
    "for filename in filenames:\n",
    "    \n",
    "    with open(blacklist_dir + filename, 'r') as backlist_file:\n",
    "        stopwords = [content.strip('\\n') for content in backlist_file.readlines()]\n",
    "        kbs_stopwords.extend(stopwords)\n",
    "        backlist_file.close()\n",
    "\n",
    "#Extend stop words vocabulary with the retrieved KBs stopwords\n",
    "all_stopwords.extend(kbs_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-channel",
   "metadata": {},
   "source": [
    "<a id=\"2.4\"></a>\n",
    "### 2.4. Extract the entities in a single file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-registration",
   "metadata": {},
   "source": [
    "Let's retrieve a file from the articles sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'data/sample/'\n",
    "\n",
    "with open(dataset_dir + '87390d2ae28407b3e03e60a6b24a7fd99ed7229a.json') as article1_file:\n",
    "    article_data = json.load(article1_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-omaha",
   "metadata": {},
   "source": [
    "Let's check the contents of the article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-bronze",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-recruitment",
   "metadata": {},
   "source": [
    "We want to recognize the entities present in title, abstract, and body. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-charger",
   "metadata": {},
   "source": [
    "First, let's retrieve the title, which is a value associated with the key 'metadata':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-country",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = article_data['metadata']['title']\n",
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-camel",
   "metadata": {},
   "source": [
    "Let's tokenize the title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tokens = word_tokenize(title)\n",
    "title_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-integrity",
   "metadata": {},
   "source": [
    "And remove the tokens relative to stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-resort",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tokens_up = [word for word in title_tokens if not word in all_stopwords]\n",
    "title_tokens_up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-location",
   "metadata": {},
   "source": [
    "And rebuild the title without the stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_up = (' ').join(title_tokens_up)\n",
    "title_up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-throw",
   "metadata": {},
   "source": [
    "Then, we apply MER to the preprocessed title in order to recognize disease entities and to link them to DO concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-television",
   "metadata": {},
   "outputs": [],
   "source": [
    "merpy.get_entities(title_up, 'do')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-introduction",
   "metadata": {},
   "source": [
    "Let's check if the annotations make sense. For instance, access the link http://purl.obolibrary.org/obo/DOID_2945."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-sound",
   "metadata": {},
   "source": [
    "The entity 'SARS' in the article was linked to the DO concept 'severe acute respiratory syndrome' with the identifier 'DOID:2945', which seems correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-spouse",
   "metadata": {},
   "source": [
    "Let's apply MER to recognize chemical entities and to link them to ChEBI concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-margin",
   "metadata": {},
   "outputs": [],
   "source": [
    "merpy.get_entities(title_up, 'chebi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-infrared",
   "metadata": {},
   "source": [
    "Accessing the link http://purl.obolibrary.org/obo/CHEBI_35341, we can see that the entity 'Steroids' was linked to the ChEBI concept 'steroid', which has the identifier 'CHEBI:35341'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-flexibility",
   "metadata": {},
   "source": [
    "We add the disease and chemical entities to a single list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_entities = merpy.get_entities(title_up, 'do') + merpy.get_entities(title_up, 'chebi')\n",
    "\n",
    "title_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-smile",
   "metadata": {},
   "source": [
    "Now, we are going to apply MER to recognize entities in abstract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = article_data['abstract'][0]['text']\n",
    "\n",
    "#Tokenize and remove stop words\n",
    "abstract_tokens = word_tokenize(abstract)\n",
    "abstract_tokens_up = [word for word in abstract_tokens if not word in all_stopwords]\n",
    "abstract_up = (' ').join(abstract_tokens_up)\n",
    "\n",
    "#Entity recognition and linking \n",
    "abstract_entities = merpy.get_entities(abstract_up, 'do') + merpy.get_entities(abstract_up, 'chebi')\n",
    "\n",
    "abstract_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-belle",
   "metadata": {},
   "source": [
    "Let's apply MER in the text associated with the body of the article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-albany",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = str()\n",
    "\n",
    "for section in article_data['body_text']:\n",
    "    body += section['text'] + \"\\n\"\n",
    "    \n",
    "#Tokenize and remove stop words\n",
    "body_tokens = word_tokenize(body)\n",
    "body_tokens_up = [word for word in body_tokens if not word in all_stopwords]\n",
    "body_up = (' ').join(body_tokens_up)\n",
    "\n",
    "#Entity recognition and linking \n",
    "body_entities = merpy.get_entities(body_up, \"do\") + merpy.get_entities(body_up, \"chebi\")\n",
    "\n",
    "body_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-kennedy",
   "metadata": {},
   "source": [
    "At last, we need to obtain information about the frequency of each ontology identifier in the document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-international",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_entities = title_entities + abstract_entities + body_entities\n",
    "\n",
    "all_uris = [entity[3] for entity in total_entities]\n",
    "\n",
    "entity_counter = Counter(all_uris)\n",
    "\n",
    "entity_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-phase",
   "metadata": {},
   "source": [
    "To sort the URIs by descending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-aquatic",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_counter = {\n",
    "    k: v \n",
    "    for k, v in sorted(entity_counter.items(), key=lambda item: item[1], reverse=True)\n",
    "    }\n",
    "\n",
    "entity_counter "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-packaging",
   "metadata": {},
   "source": [
    "<a id=\"2.5\"></a>\n",
    "### 2.5. Create entity files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-cancer",
   "metadata": {},
   "source": [
    "We need to adapt our code to perform NER and NEL in all documents of our sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-timber",
   "metadata": {},
   "source": [
    "First, create the output dir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-condition",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'data/sample_entities/'\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-badge",
   "metadata": {},
   "source": [
    "Next, we are going to iterate over each file present in the sample directory, annotate it, and create the respective entity file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-sponsorship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_doc(article):\n",
    "    \n",
    "    article_filepath = 'data/sample/' + article  \n",
    "\n",
    "    with open(article_filepath) as input_file:\n",
    "        article_data = json.load(input_file)\n",
    "\n",
    "    doc_output = {'id': str(), 'entities': {}, 'sections': {'title': [], 'abstract': [], 'body': []}}\n",
    "    \n",
    "    doc_output['id'] = article_data['paper_id']\n",
    "   \n",
    "    #Annotate the title\n",
    "    title = article_data['metadata']['title']\n",
    "    \n",
    "    #Tokenize and remove stop words\n",
    "    title_tokens = word_tokenize(title)\n",
    "    title_tokens_up = [word for word in title_tokens if not word in all_stopwords]\n",
    "    title_up = (' ').join(title_tokens_up)\n",
    "    \n",
    "    #Entity recognition and linking\n",
    "    title_entities = merpy.get_entities(title_up, 'do') + merpy.get_entities(title_up, 'chebi')\n",
    "    doc_output['sections']['title'] = title_entities\n",
    "    \n",
    "    #Annotate the abstract\n",
    "    if article_data['abstract'] != []:\n",
    "        abstract = article_data['abstract'][0]['text']\n",
    "        \n",
    "        #Tokenize and remove stop words\n",
    "        abstract_tokens = word_tokenize(abstract)\n",
    "        abstract_tokens_up = [word for word in abstract_tokens if not word in all_stopwords]\n",
    "        abstract_up = (' ').join(abstract_tokens_up)\n",
    "        \n",
    "        #Entity recognition and linking\n",
    "        abstract_entities = merpy.get_entities(abstract_up, 'do') + merpy.get_entities(abstract_up, 'chebi')\n",
    "        doc_output['sections']['abstract'] = abstract_entities\n",
    "    \n",
    "    else:\n",
    "        abstract_entities = []\n",
    "\n",
    "    #Combine the several paragraphs of the body text and annotate it\n",
    "    body = str()\n",
    "\n",
    "    for section in article_data['body_text']:\n",
    "        body += section['text'] + '\\n'\n",
    "    \n",
    "    #Tokenize and remove stop words\n",
    "    body_tokens = word_tokenize(body)\n",
    "    body_tokens_up = [word for word in body_tokens if not word in all_stopwords]\n",
    "    body_up = (' ').join(body_tokens_up)    \n",
    "        \n",
    "    #Entity recognition and linking\n",
    "    body_entities = merpy.get_entities(body_up, 'do') + merpy.get_entities(body_up, 'chebi')\n",
    "    doc_output['sections']['body'] = body_entities\n",
    "\n",
    "    # Count URIs frequencies and sort them\n",
    "    total_entities = title_entities + abstract_entities + body_entities\n",
    "    all_uris = [entity[3] for entity in total_entities if len(entity)==4]\n",
    "    entity_counter = Counter(all_uris)\n",
    "    \n",
    "    doc_output['entities'] = {\n",
    "        k: v \n",
    "        for k, v in sorted(entity_counter.items(), key=lambda item: item[1], reverse=True)\n",
    "        }\n",
    "    \n",
    "    #Generate JSON file with output\n",
    "    out_filepath = out_dir + doc_output['id'] + '_entities.json'\n",
    "    \n",
    "    with open(out_filepath, 'w') as out_file:\n",
    "        out_file.write(json.dumps(doc_output, indent=4))\n",
    "        \n",
    "\n",
    "article_dir = 'data/sample/'\n",
    "        \n",
    "with multiprocessing.Pool(processes=10) as pool: \n",
    "    # change the number of processes according to number of available cores\n",
    "    outputs = pool.map(annotate_doc, [article for article in os.listdir(article_dir)], chunksize=10)\n",
    "    pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-scheme",
   "metadata": {},
   "source": [
    "If you were not able to run the previous code, you can extract the file 'sample_entities.tar.xz' under 'data' directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-yield",
   "metadata": {},
   "source": [
    "Now we have both the article files ('data/sample' directory) and the respective entities files ('data/sample_entities' directory), and the next step will be the generation of the scientific recomendation dataset using the LIBRETTI algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-terrorism",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. Create the recommendation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-printing",
   "metadata": {},
   "source": [
    "<a id=\"3.1\"></a>\n",
    "### 3.1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-publisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import date, datetime\n",
    "\n",
    "from pathlib import Path\n",
    "import rdflib\n",
    "from rdflib import URIRef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-sperm",
   "metadata": {},
   "source": [
    "<a id=\"3.2\"></a>\n",
    "### 3.2. Import user-defined-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-straight",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.utils import *\n",
    "from util.utils2json import *\n",
    "from util.functions import *\n",
    "from util.utils2ontologies import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"max_rows\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-mathematics",
   "metadata": {},
   "source": [
    "Entities defined by user, our example will work with chebi and doid ontologies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-timber",
   "metadata": {},
   "source": [
    "Path of original' json and entities' json folder, blacklist and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_json_folder = \n",
    "entities_json_folder = \n",
    "path_to_metadata = 'data/metadata.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a list containing the names of the entries in the  directory given by path\n",
    "entities_list_of_json_files = os.listdir('data/sample_entities/')\n",
    "#print(entities_list_of_json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Get all articles id that cannot be considered in use case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-thermal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return all articles to be removed, due some errors found there\n",
    "articles_blacklist = []\n",
    "with open('data/blacklist/blacklist_articles.txt', 'r') as f:\n",
    "    black_list = [content for content in f.readlines()]\n",
    "articles_blacklist.extend(black_list) \n",
    "f.close()\n",
    "\n",
    "print(articles_blacklist)\n",
    "metadata = pd.read_csv('data/metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_rating_all = []\n",
    "count = 0\n",
    "#entities_list_of_json_files=['PMC7176216_entities.json']\n",
    "for file in entities_list_of_json_files:\n",
    "    # ## Exception\n",
    "    if file.replace('_entities.json','') in articles_blacklist:\n",
    "        continue\n",
    "    # if file.startswith('PMC'):\n",
    "    #     continue    \n",
    "    print(count, \"-\", len(entities_list_of_json_files))            \n",
    "    print(file)\n",
    "        \n",
    "    # check valid json file, i.e. contains values\n",
    "    try:\n",
    "        j_file_entities = pd.read_json(entities_json_folder + file, orient = 'index')     \n",
    "    except Exception as e:\n",
    "        print(f'Json file does not contain values. Error message {e}')\n",
    "        #This function receives the article to and saves them in the\n",
    "        # blacklist file. The backlist contains all invalid articles, such\n",
    "        # as non-authors, non-entities, and others\n",
    "        with open('data/blacklist/blacklist_articles.txt', 'r+') as blacklist_file:\n",
    "            content = blacklist_file.read()\n",
    "            blacklist_file.seek(0, 0)\n",
    "            blacklist_file.write(file.replace('_entities.json','').rstrip('\\r\\n') + '\\n')\n",
    "            blacklist_file.write(content)\n",
    "            blacklist_file.close()\n",
    "        continue\n",
    "    \n",
    "    '''\n",
    "    CHECK IF IS NECESSARY\n",
    "    df_entities = get_entities_id(get_entities(j_file_entities))\n",
    "    if df_entities.empty:\n",
    "        print(f'Json file does not contain values.')\n",
    "        set_blacklist(file=path_to_blacklist, line=file.replace('_entities.json',''))\n",
    "        continue    \n",
    "    \n",
    "    print(df_entities)'''\n",
    "    article_id = j_file_entities.loc['id'].values[0]\n",
    "    \n",
    "    # check valid json file, i.e. contains values\n",
    "    try:\n",
    "        # Convert a JSON string to pandas object, and return a dataframe\n",
    "        j_file_original = pd.read_json('data/sample/' + article_id, orient = 'index')\n",
    "        #print(j_file_original)\n",
    "    except Exception as e:\n",
    "        print(f'Original json file does not exist. Error message {e}')\n",
    "        #This function receives the article to and saves them in the\n",
    "        # blacklist file. The backlist contains all invalid articles, such\n",
    "        # as non-authors, non-entities, and others\n",
    "        with open('data/blacklist/blacklist_articles.txt', 'r+') as blacklist_file:\n",
    "            content = blacklist_file.read()\n",
    "            blacklist_file.seek(0, 0)\n",
    "            blacklist_file.write(file.replace('_entities.json','').rstrip('\\r\\n') + '\\n')\n",
    "            blacklist_file.write(content)\n",
    "            blacklist_file.close()\n",
    "        continue    \n",
    "    \n",
    "    # check if json file contains authors, otherwise try to find them in metadata.csv\n",
    "    # if value remains null them put this article in the blacklist file\n",
    "    list_of_authors = []\n",
    "    for p in j_file_original['metadata']['authors']:\n",
    "        \n",
    "        if len(p['first']) == 0 or len(p['last']) == 0:\n",
    "           continue\n",
    "        else:\n",
    "            # remove all characters except alphabets from a string to unidecode\n",
    "            first = unidecode.unidecode( ''.join(m for m in p['first'] if m.isalpha()))\n",
    "            middle = unidecode.unidecode( ''.join(m for m in p['middle'] if m.isalpha()))\n",
    "            last = unidecode.unidecode( ''.join(m for m in p['last'] if m.isalpha()))\n",
    "\n",
    "            list_of_authors.append(first + ', '+ middle + ' '+ last)\n",
    "    \n",
    "###  ---- Metadata ----\n",
    "    # Some datas return an array, then we convert to a list and get first element\n",
    "    publish_date=''      \n",
    "    try:\n",
    "        publish_date = metadata[metadata.sha == article_id].\\\n",
    "            publish_time.map(lambda v: v.split('-')[0]).tolist()[0]\n",
    "    finally:\n",
    "        pass  \n",
    "    print(publish_date)\n",
    "    if publish_date==None:\n",
    "        #This function receives the article to and saves them in the\n",
    "        # blacklist file. The backlist contains all invalid articles, such\n",
    "        # as non-authors, non-entities, and others\n",
    "        with open('data/blacklist/blacklist_articles.txt', 'r+') as blacklist_file:\n",
    "            content = blacklist_file.read()\n",
    "            blacklist_file.seek(0, 0)\n",
    "            blacklist_file.write(file.replace('_entities.json','').rstrip('\\r\\n') + '\\n')\n",
    "            blacklist_file.write(content)\n",
    "            blacklist_file.close()\n",
    "        continue\n",
    "    \n",
    "    # if authors is empty we will find in metadata.csv file\n",
    "    if len(list_of_authors)==0:\n",
    "        ##if string is NaN\n",
    "        try:\n",
    "            authors = data[metadata.sha == article_id].authors.values[0].split(';')\n",
    "            for a in authors:\n",
    "                a = a.split(',')\n",
    "                first = unidecode.unidecode( ''.join(m for m in a[1] if m.isalpha()))\n",
    "                last = unidecode.unidecode( ''.join(m for m in a[0] if m.isalpha()))\n",
    "                list_of_authors.append(first + ', '+  last) \n",
    "        except Exception as e:\n",
    "            print(f'Empty values {e}')   \n",
    "            #This function receives the article to and saves them in the\n",
    "            # blacklist file. The backlist contains all invalid articles, such\n",
    "            # as non-authors, non-entities, and others\n",
    "            with open('data/blacklist/blacklist_articles.txt', 'r+') as blacklist_file:\n",
    "                content = blacklist_file.read()\n",
    "                blacklist_file.seek(0, 0)\n",
    "                blacklist_file.write(file.replace('_entities.json','').rstrip('\\r\\n') + '\\n')\n",
    "                blacklist_file.write(content)\n",
    "                blacklist_file.close()               \n",
    "        continue\n",
    "    count+=1\n",
    "    \n",
    "###  ---- End of Metadata ----            \n",
    "    \n",
    "    # Append <user, item, rating> tuple\n",
    "    user_item_rating = []\n",
    "    for author in list_of_authors:\n",
    "        for entity in df_entities.entities_id:\n",
    "            user_item_rating.append([author, entity, 1])\n",
    "\n",
    "    #print(user_item_rating)\n",
    "    \n",
    "    ## add publish_date in array in index column = 3\n",
    "    user_item_rating = np.insert(user_item_rating, 3, publish_date, axis=1)\n",
    "    # print(user_item_rating)\n",
    "    user_item_rating_all.append(user_item_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = []\n",
    "for sublist in user_item_rating_all:\n",
    "    for item in sublist:\n",
    "        flat_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.DataFrame(np.array(flat_list),  columns=['user', 'item', 'rating', 'year'])\n",
    "sum_df = final_data.groupby(['user', 'item', 'year']).size().reset_index().rename(columns={0: 'rating'})   \n",
    "#     maps the values to the lowest consecutive values\n",
    "df_user_index = pd.DataFrame(sum_df.user.unique(), columns=[\"user\"])\n",
    "df_user_index[\"new_index\"] = np.arange(0, len(sum_df.user.unique()))\n",
    "\n",
    "sum_df[\"index_user\"] = sum_df[\"user\"].map(df_user_index.set_index('user')[\"new_index\"]).fillna(0) \n",
    "df_with_user_id = sum_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-greece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# swap columns: user and index_user, and after rename to author_name\n",
    "#df_with_user_id['index_user'], df_with_user_id['user'] = df_with_user_id['user'], df_with_user_id['index_user']\n",
    "df_with_user_id.rename(columns={'user': 'author_name', 'index_user': 'user'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-representation",
   "metadata": {},
   "source": [
    "<a id=\"3.3\"></a>\n",
    "### 3.3. Get entities labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_entities = df_with_user_id.item.unique()\n",
    "print(list_of_entities) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-austria",
   "metadata": {},
   "source": [
    "<a id=\"3.4\"></a>\n",
    "### 3.4. Loading ontologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_chebi = 'data/ontologies/chebi.owl'\n",
    "path_do = 'data/ontologies/doid.owl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-quality",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Get entities lables from http://purl.obolibrary.org/obo/ based on items prefix\n",
    "\n",
    "entities_label = []\n",
    "for id in list_of_entities: \n",
    "    uri = URIRef('http://purl.obolibrary.org/obo/' + id)\n",
    "    if id.startswith('CHEBI'):\n",
    "        label = chebi.label(uri)\n",
    "    elif id.startswith('DO'):\n",
    "        label = do.label(uri)\n",
    "    entities_label.append(label)\n",
    "\n",
    "print(entities_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-jerusalem",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entities = pd.DataFrame(list_of_entities, columns=[\"item_id\"])\n",
    "df_entities[\"entity_name\"] = np.array(entities_label)\n",
    "print(df_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mapping labels')\n",
    "df_with_user_id[\"item_name\"] = df_with_user_id[\"item\"].map(df_entities.set_index('item_id')[\"entity_name\"]).fillna(0)\n",
    "print(df_with_user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-skating",
   "metadata": {},
   "source": [
    "<a id=\"3.5\"></a>\n",
    "### 3.5. Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-avatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_cord_ds = 'data/results/sample_cord-19_dataset.csv'\n",
    "path_to_cord_userid = 'data/results/sample_cord-19_dataset_userid.csv'\n",
    "print('saving data')\n",
    "if not os.path.exists(os.path.dirname('data/results/sample_cord-19_dataset.csv')):\n",
    "        Path(os.path.dirname('data/results/sample_cord-19_dataset.csv')).mkdir(parents=True, exist_ok=True)   \n",
    "df_with_user_id[['user', 'item', 'rating', 'item_name', 'year']].to_csv('data/results/sample_cord-19_dataset.csv', index=False, header=False)\n",
    "\n",
    "if not os.path.exists(os.path.dirname('data/results/sample_cord-19_dataset_userid.csv')):\n",
    "        Path(os.path.dirname('data/results/sample_cord-19_dataset_userid.csv')).mkdir(parents=True, exist_ok=True)   \n",
    "df_with_user_id[['user', 'author_name']].to_csv('data/results/sample_cord-19_dataset_userid.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-being",
   "metadata": {},
   "source": [
    "### 4.0. Data statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('data/results/sample_cord-19_dataset.csv', names=['user', 'item', 'rating', 'item_name', 'year'])\n",
    "#dataset = pd.read_csv('/home/mbarros/Desktop/data/scirec2021/comm_subset_cord-19_dataset_total_ordered_filtered.csv', names=['user', 'item', 'rating', 'item_name', 'year'])\n",
    "\n",
    "\n",
    "print(dataset.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-insured",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique users\n",
    "# number of unique items\n",
    "# number of ratings\n",
    "# sparsity \n",
    "\n",
    "print('n users: ', dataset.user.unique().shape[0])\n",
    "print('n items: ', dataset.item.unique().shape[0])\n",
    "print('n ratings: ', dataset.size)\n",
    "\n",
    "print('sparsity: ', 1 - (dataset.size / (dataset.user.unique().shape[0] * dataset.item.unique().shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-lewis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# items by ontology\n",
    "\n",
    "print(\"CHEBI items: \", dataset[dataset.item.str.startswith('CHEBI')].item.unique().shape[0])\n",
    "\n",
    "print(\"DOID items: \", dataset[dataset.item.str.startswith('DOID')].item.unique().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-jordan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# items by user\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "unique_users = dataset.user.unique()\n",
    "items_by_user = dataset.groupby(['user'])[\"item\"].count().reset_index()\n",
    "items_by_user = items_by_user.sort_values(by=['item'], ascending=False)\n",
    "print(items_by_user)\n",
    "items_by_user.user = items_by_user.user.astype('str')\n",
    "\n",
    "print('max items by user: ', items_by_user.item.max())\n",
    "print('min items by user: ', items_by_user.item.min())\n",
    "print('mean items by user: ', items_by_user.item.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-armstrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(items_by_user.user, items_by_user.item)\n",
    "plt.axhline(y=items_by_user.item.mean(), color='r', linestyle='-')\n",
    "plt.ylabel('number of items')\n",
    "plt.xlabel('user')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# users by item\n",
    "\n",
    "unique_items = dataset.item.unique()\n",
    "users_by_item = dataset.groupby(['item'])[\"user\"].count().reset_index()\n",
    "users_by_item = users_by_item.sort_values(by=['user'], ascending=False)\n",
    "\n",
    "print('max users by item: ', users_by_item.user.max())\n",
    "list_of_max_items = users_by_item[users_by_item.user == users_by_item.user.max()].item.values\n",
    "print(list_of_max_items)\n",
    "\n",
    "print(dataset[dataset.item == list_of_max_items[0]])\n",
    "\n",
    "print('min users by item: ', users_by_item.user.min())\n",
    "print('mean users by item: ', users_by_item.user.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-heath",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_doid = dataset[dataset.item.str.startswith('DOID')]\n",
    "unique_items_d = dataset_doid.item.unique()\n",
    "users_by_item_d = dataset_doid.groupby(['item'])[\"user\"].count().reset_index()\n",
    "users_by_item_d = users_by_item_d.sort_values(by=['user'], ascending=False)\n",
    "\n",
    "print('max users by item: ', users_by_item_d.user.max())\n",
    "list_of_max_items_d = users_by_item_d[users_by_item_d.user == users_by_item_d.user.max()].item.values\n",
    "print(list_of_max_items_d)\n",
    "\n",
    "print(dataset_doid[dataset_doid.item == list_of_max_items_d[0]])\n",
    "\n",
    "print('min users by item: ', users_by_item_d.user.min())\n",
    "print('mean users by item: ', users_by_item_d.user.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-grocery",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_chebi = dataset[dataset.item.str.startswith('CHEBI')]\n",
    "unique_items_c = dataset_chebi.item.unique()\n",
    "users_by_item_c = dataset_chebi.groupby(['item'])[\"user\"].count().reset_index()\n",
    "users_by_item_c = users_by_item_c.sort_values(by=['user'], ascending=False)\n",
    "\n",
    "print('max users by item: ', users_by_item_c.user.max())\n",
    "list_of_max_items_c = users_by_item_c[users_by_item_c.user == users_by_item_c.user.max()].item.values\n",
    "print(list_of_max_items_c)\n",
    "\n",
    "print(dataset_chebi[dataset_chebi.item == list_of_max_items_c[0]])\n",
    "\n",
    "print('min users by item: ', users_by_item_c.user.min())\n",
    "print('mean users by item: ', users_by_item_c.user.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib qt\n",
    "\n",
    "plt.scatter(users_by_item.item, users_by_item.user)\n",
    "plt.axhline(y=users_by_item.user.mean(), color='r', linestyle='-')\n",
    "#plt.yscale('log')\n",
    "plt.ylabel('number of users')\n",
    "plt.xlabel('items')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys_scifi",
   "language": "python",
   "name": "recsys_scifi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
